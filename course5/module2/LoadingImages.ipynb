{"cells":[{"cell_type":"markdown","id":"8b2e8f6b-f14a-4920-862b-1bcd0bb59ac6","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n","</center>\n"]},{"cell_type":"markdown","id":"61cd766e-a5f1-444e-b3f1-d68dec8615ca","metadata":{},"outputs":[],"source":["# **Loading Images in Keras**\n"]},{"cell_type":"markdown","id":"6927e80a-3317-4061-afd5-f4ca8bacf021","metadata":{},"outputs":[],"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","id":"59558530-72d1-4275-a627-b67009852df7","metadata":{},"outputs":[],"source":["<h1> CNN Training Pipeline </h1></s>\n"]},{"cell_type":"markdown","id":"1a97d12b-64a6-40fa-9d82-e1f2682bf8db","metadata":{},"outputs":[],"source":["Convolutional neural networks (CNNs) have had great success in certain kinds of problems, such as image recognition. Data loading and preparation are important steps when it comes to working with such models. Increasingly, data augmentation is also required for more complex object recognition tasks. In this lab, we will discover various ways of loading images, as well as converting, augmenting and saving image datasets using the Keras API.\n"]},{"cell_type":"markdown","id":"ab7d3611-f84e-480f-bcb4-5d9f6fadc117","metadata":{},"outputs":[],"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/cnn_image_loading_lab.drawio.png)\n"]},{"cell_type":"markdown","id":"7440604a-ea20-4149-a95e-386fa78f2c82","metadata":{},"outputs":[],"source":["### What's a Digital Image?\n"]},{"cell_type":"markdown","id":"50d818e9-cc83-4ebe-bd71-04c5a8d81ed7","metadata":{},"outputs":[],"source":["A gray-scale digital image can be interpreted as a rectangular array of numbers. If we zoom into the region, we see the image is comprised of a rectangular grid of discrete blocks called pixels. We can represent these pixels with numbers called intensity values ranging from 0 to 255, as shown here:\n"]},{"cell_type":"markdown","id":"26187a1c-793c-4791-9d5c-545d78e376da","metadata":{},"outputs":[],"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/Gray_scale_lana.png)\n"]},{"cell_type":"markdown","id":"ad5357f3-4f47-43c7-8612-2daeac481202","metadata":{},"outputs":[],"source":["Color images are a combination of red, blue, and green intensity values as shown here\n"]},{"cell_type":"markdown","id":"0c89c132-4496-4c51-8462-4aa9d7e9edd5","metadata":{},"outputs":[],"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/RGB_ape.png)\n"]},{"cell_type":"markdown","id":"e2446605-1994-4eec-9d60-12bf1445677a","metadata":{},"outputs":[],"source":["each channel has  values ranging from 0 to 255.\n"]},{"cell_type":"markdown","id":"11b7314f-927b-48f5-8b2e-54982b5a95af","metadata":{},"outputs":[],"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/RGB_array.png)\n"]},{"cell_type":"markdown","id":"0673f9ad-56ed-415d-a897-39dc68126a5f","metadata":{},"outputs":[],"source":["Finally, to apply a neural network to Karis we sometimes add a batch dimension, this is just  an extra dimension. Just think of the batch dimension as an address that contains an image array.\n"]},{"cell_type":"markdown","id":"a10dd042-da9b-4d71-83a6-61b246800001","metadata":{},"outputs":[],"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/batch_dim_images.png)\n"]},{"cell_type":"markdown","id":"a3297d4a-615b-46ed-a6d1-15eb71421498","metadata":{},"outputs":[],"source":["## **Table of Contents**\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","    </li>\n","    <li><a href=\"https://#Background\">Objectives</a></li>\n","    <li>\n","        <a href=\"https://#Ways to load images\">Ways to load images</a>\n","        <ol>\n","            <li><a href=\"https://#Ready-to-use toy datasets\">Ready-to-use toy datasets</a></li>\n","            <li><a href=\"https://#Load individual images as PIL objects\">Load individual images as PIL objects</a></li>\n","            <li><a href=\"https://#Load from directory of images\">Load from directory of images</a></li>\n","            <li><a href=\"https://#Load image from URL\">Load image from URL</a></li>\n","        </ol>\n","    </li>\n","</ol>\n"]},{"cell_type":"markdown","id":"01211f03-300e-48d6-b18c-d2de7e22f271","metadata":{},"outputs":[],"source":["<h1 href=\"Objectives\">Objectives</h1>\n","\n","After completing this lab you will be able to **apply** Keras to:\n","\n","*   Load and display images in multiple ways\n","*   Convert between array and PIL formats\n","*   Convert images to grayscale, augment images through transformations and save images to files\n"]},{"cell_type":"markdown","id":"11c0f04d-8a3b-4f23-8846-98a84a9d1bb8","metadata":{},"outputs":[],"source":["***\n"]},{"cell_type":"markdown","id":"30b2dd49-6ecc-4a0f-9651-94f4c0654507","metadata":{},"outputs":[],"source":["<h2 href=\"Setup\">Setup</h2>\n"]},{"cell_type":"markdown","id":"12736b40-3dba-4482-b188-ed9ec7f9d8af","metadata":{},"outputs":[],"source":["For this lab, we will be using the following libraries:\n","\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`Pillow`](https://pillow.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for image processing functions.\n","*   [`OpenCV`](https://docs.opencv.org/4.x/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for other image processing functions.\n","*   [`tensorflow`](https://www.tensorflow.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and neural network related functions.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","*   [`keras`](https://keras.io/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for building artificial neural networks.\n"]},{"cell_type":"markdown","id":"e93f0d78-a470-47b9-8c09-7bfff7a08708","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","id":"068ab3aa-5968-4eeb-9add-3721278b9529","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n#!pip install numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 opencv-python==4.5.5.62 keras\n\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n\n# RESTART YOUR KERNEL AFTERWARD AS WELL"]},{"cell_type":"code","id":"5f9add5b-c4cb-4512-92ef-11d128d18f2d","metadata":{},"outputs":[],"source":["!pip3 install --upgrade tensorflow\n\n# RESTART YOUR KERNEL AFTERWARD AS WELL"]},{"cell_type":"markdown","id":"c0536fc9-bc6c-4272-abe8-bcfd19f37d19","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","\n","*We recommend you import all required libraries in one place (here):*\n"]},{"cell_type":"code","id":"376893ee-ebd1-4359-9668-9d3fa27207b9","metadata":{},"outputs":[],"source":["# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\n\n\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # tensorflow INFO and WARNING messages are not printed "]},{"cell_type":"code","id":"26d53daa-9371-44ee-8fa0-02a952b58666","metadata":{},"outputs":[],"source":["import random \n\nimport pathlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport PIL\nimport PIL.Image\nfrom PIL import Image, ImageOps\nimport tensorflow as tf\n\nimport keras\nfrom keras.preprocessing import image\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import load_img \nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n\nimport cv2\n\nsns.set_context('notebook')\nsns.set_style('white')"]},{"cell_type":"markdown","id":"927a20a6-8f64-4cf5-9ed0-930e1697b6a6","metadata":{},"outputs":[],"source":["### Defining Helper Functions\n"]},{"cell_type":"code","id":"36577186-bd1e-4f0a-919c-8b45e4aabcae","metadata":{},"outputs":[],"source":["# This function will allow us to visualize four sample images from the loaded toy image dataset. \ndef visualize(X_train):\n    plt.rcParams['figure.figsize'] = (6,6) \n\n    for i in range(4):\n        plt.subplot(2,2,i+1)\n        num = random.randint(0, len(X_train))\n        plt.imshow(X_train[num], cmap='gray', interpolation='none', vmin=0, vmax=255)\n        plt.title(\"class {}\".format(y_train[num]))\n    \n    plt.tight_layout()"]},{"cell_type":"markdown","id":"581e0b5e-c648-4ca1-83dd-7ef7d1fe1cb6","metadata":{},"outputs":[],"source":["## Background\n"]},{"cell_type":"markdown","id":"f1d4d1cc-b8d7-400e-acb2-9127a5aa4243","metadata":{},"outputs":[],"source":["Keras is an open-source Python library used for developing and evaluating deep learning models. It provides utilities for loading, preparing, converting, augmenting, and saving image data. In this lab, we will explore various ways of loading image datasets in Keras.\n"]},{"cell_type":"code","id":"9aea749f-00ae-4497-9997-4aa81f584991","metadata":{},"outputs":[],"source":["# Print tensorflow version\nprint(tf.__version__)\n\n# Try !pip install --upgrade tensorflow if the version printed\n# is less than 2.9.0"]},{"cell_type":"markdown","id":"fa85d8c9-b632-4e92-996f-0c2dccb8e05f","metadata":{},"outputs":[],"source":["## Ways to load images\n"]},{"cell_type":"markdown","id":"ead8693f-571d-4128-8802-822e51f5ece8","metadata":{},"outputs":[],"source":["We will look into four main ways of using image datasets in Keras:\n","\n","*   Loading in a ready-to-use toy dataset from [Keras](https://keras.io/api/datasets/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n","*   Loading individual images as PIL objects\n","*   Reading a directory of images on disk using `tf.keras.utils.image_dataset_from_directory`.\n","*   Loading image from URL\n"]},{"cell_type":"markdown","id":"b5db41ad-c195-4742-b7da-067dc33818ce","metadata":{},"outputs":[],"source":["### A. Ready-to-use toy datasets\n"]},{"cell_type":"markdown","id":"1a68620e-0e68-4096-b22b-329a161ee897","metadata":{},"outputs":[],"source":["The `tf.keras.datasets` in Keras provide a few in-built image datasets that have been cleaned and are typically helpful in debugging models, or creating simple examples.\n","\n","These include MNIST hand-written digits, Fashion MNIST, CIFAR10, and CIFAR100.\n"]},{"cell_type":"markdown","id":"01fbb4b0-c3d4-49b6-831e-492dca6139f3","metadata":{},"outputs":[],"source":["**MNIST hand-written digits** is a collection of 60,000 28x28 grayscale images belonging to 10 different classes, along with a test set of 10,000 images. Let us load in the MNIST hand-written digits dataset using the `load_data` function.\n"]},{"cell_type":"code","id":"fcb700e7-1bcf-495a-8855-d447ca6b86e3","metadata":{},"outputs":[],"source":["(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"]},{"cell_type":"markdown","id":"0470e641-37fe-4cbd-9990-6ca727e5c7ab","metadata":{},"outputs":[],"source":["X_train is a uint8 numpy array of grayscale images with shapes (60000, 28, 28) from the training dataset. Its pixel values range from 0 to 255.\n","\n","y_train is a uint8 numpy array of digit labels (integers in range 0-9) with shape (60000) for the training data.\n"]},{"cell_type":"code","id":"13c93b16-9e64-44c6-a0aa-7853b93c5708","metadata":{},"outputs":[],"source":["print(X_train.shape)\nprint(X_train.dtype)\nprint(y_train.shape)\nprint(y_train.dtype)"]},{"cell_type":"markdown","id":"43d3b2b9-56c7-4ac7-9428-c620db59dd9f","metadata":{},"outputs":[],"source":["Let's visualize four random images from the training set. Only one color channel is needed to display the image as a grayscale. So for this step, we take a single color channel and display the image using the `plt.imshow()` method with `cmap` set to `gray`, `vmin` set to 0, and `vmax` set to 255.\n"]},{"cell_type":"code","id":"4935bccd-496e-4794-8d64-440da77dd8fb","metadata":{},"outputs":[],"source":["visualize(X_train)"]},{"cell_type":"markdown","id":"0c585ff6-5c36-46fb-8aab-6e41857d3569","metadata":{},"outputs":[],"source":["#### Using image data with Autoencoders\n"]},{"cell_type":"markdown","id":"e42281e1-ffd7-48d6-93b6-64ae27b939ab","metadata":{},"outputs":[],"source":["As you will see in future labs, an autoencoder (AE) is a neural network where the input is the same as the output.  \n","\n","To build an AE, you need three things:\n","\n","* an encoding function\n","* a decoding function\n","* distance function between the amount of information loss between the compressed representation of your data and the decompressed representation, that is, a loss function\n","\n","Some practical applications of AEs are data denoising and dimensionality reduction for data visualization.\n"]},{"cell_type":"markdown","id":"7fb3d50c-2212-48c5-844e-580bbf1acc36","metadata":{},"outputs":[],"source":["We can define a size for our encoded representations, and use `Dense()` to create encoded representation of the input. Similarly, we can use `Dense()` to create lossy reconstructions of the input. As seen below, `encoded` has a shape of (None, 64) and `reconstruction` has the original input size of (None, 784).\n"]},{"cell_type":"code","id":"9edc5028-8366-4189-9b77-9ee938403ff5","metadata":{},"outputs":[],"source":["ENCODING_DIM = 64\n\n# Encoded representations:\ninputs = Input(shape=(784,)) \nencoded = Dense(ENCODING_DIM, activation=\"sigmoid\")(inputs)\n\n# Reconstructions:\nencoded_inputs = Input(shape=(ENCODING_DIM,), name='encoding')\nreconstruction = Dense(784, activation=\"sigmoid\")(encoded_inputs)\n\nprint(\"Encoded Input: \", encoded.shape)\nprint(\"Reconstructed Input: \", reconstruction.shape)"]},{"cell_type":"markdown","id":"6ebec7af-028c-4db7-908c-40d2a7ebde4d","metadata":{},"outputs":[],"source":["Using this technique, we can instantiate three types of models:\n","\n","* End-to-end AEs mapping inputs to reconstructions\n","* An encoder mapping inputs to the latent space\n","* A decoder that takes in points from latent space and output corresponding reconstructed samples\n"]},{"cell_type":"markdown","id":"8539c787-0f84-474f-af80-eda4358a89c2","metadata":{},"outputs":[],"source":["### Exercise 1 - Load Fashion MNIST\n"]},{"cell_type":"markdown","id":"b69a65dc-ae6a-41f5-a167-6c98830b45ee","metadata":{},"outputs":[],"source":["**Fashion MNIST** is another collection of 60,000 28x28 grayscale images belonging to 10 different classes, along with a test set of 10,000 images. Write code to load the Fashion MNIST dataset using  `fashion_mnist.load_data()` from `keras.datasets`.\n"]},{"cell_type":"code","id":"b3668ca1-debc-4b03-b5d4-1dcd379ef585","metadata":{},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","id":"259be6fb-d799-4bb6-91a7-c77b1ece643f","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for solution</summary>\n","\n","```python\n","\n","(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"64161fa6-294d-44f3-8efd-d5d04d22ee3a","metadata":{},"outputs":[],"source":["X_train is a uint8 numpy array of grayscale images with shapes (60000, 28, 28) from the training dataset. Its pixel values range from 0 to 255.\n","\n","y_train is a uint8 numpy array of digit labels (integers in range 0-9) with shape (60000,) for the training data.\n"]},{"cell_type":"markdown","id":"97775c31-4982-4cc5-8c5a-85158af499df","metadata":{},"outputs":[],"source":["Now print the shape and dtype of the X_train and y_train arrays.\n"]},{"cell_type":"code","id":"94705cb1-0f31-480c-93d3-51e38cb4894a","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"42205de8-fd01-41fe-ac73-b1d4e5eef6d5","metadata":{},"outputs":[],"source":["Display a few randomly selected images as grayscale using the `visualize()` helper function.\n"]},{"cell_type":"code","id":"0f586883-973d-4066-a76b-f68905f65f34","metadata":{},"outputs":[],"source":["# Write your code here\n# Visualize a few images from the training set\n\n"]},{"cell_type":"markdown","id":"7249d34c-840f-4351-adc7-c05e6779c528","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","\n","```python\n","visualize(X_train) \n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"9c44525b-232c-48b6-afd0-5b62dec55d3c","metadata":{},"outputs":[],"source":["**CIFAR10** is a collection of 50,000 32x32 color training images labeled over 10 different categories, along with a test set of 10,000 images.\n"]},{"cell_type":"code","id":"ee014722-3eee-410c-9167-2b830e2bfd55","metadata":{},"outputs":[],"source":["(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"]},{"cell_type":"markdown","id":"fb34c2ab-2224-4537-9c3d-7a9ea2cfb9d2","metadata":{},"outputs":[],"source":["X_train is a uint8 numpy array of grayscale image data with shapes (50000, 32, 32, 3), containing the training data. Pixel values range from 0 to 255.\n","\n","y_train is a uint8 NumPy array of labels (integers in range 0-9) with shape (50000, 1) for the training data.\n"]},{"cell_type":"code","id":"a4374e32-83ef-4cd3-982c-e39f329cab0b","metadata":{},"outputs":[],"source":["print(X_train.shape)\nprint(X_train.dtype)"]},{"cell_type":"code","id":"b1e5ec9e-f2ea-42ba-b9ec-3d747939c201","metadata":{},"outputs":[],"source":["visualize(X_train)"]},{"cell_type":"markdown","id":"64308111-080a-4761-8180-1929af6f7a4e","metadata":{},"outputs":[],"source":["**CIFAR100** is a collection of 50,000 32x32 color training images labeled over 100 fine-grained classes and 20 coarse-grained classes, along with a test set of 10,000 images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (label) and a \"coarse\" label (superclass).\n"]},{"cell_type":"markdown","id":"5dd007cb-783f-46fe-9a59-50389685ba1a","metadata":{},"outputs":[],"source":["Using the fine `label_mode`:\n"]},{"cell_type":"code","id":"bf545ecf-9714-4538-8546-8aaefc06dd2a","metadata":{},"outputs":[],"source":["(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data(label_mode = 'fine')"]},{"cell_type":"markdown","id":"8a02b241-3fe4-4f1c-bdd0-752c6c0b14d7","metadata":{},"outputs":[],"source":["X_train is a uint8 numpy array of grayscale image data with shapes (50000, 32, 32, 3), containing the training data. Pixel values range from 0 to 255.\n","\n","y_train is a uint8 numpy array of labels (integers in range 0-99) with shape (50000, 1) for the training data.\n"]},{"cell_type":"code","id":"77b6f830-dbc3-4a9e-8208-364f2bde1538","metadata":{},"outputs":[],"source":["print(X_train.shape)\nprint(X_train.dtype)"]},{"cell_type":"code","id":"6d20e236-d1eb-4e9d-b4d7-4b2b3f995c21","metadata":{},"outputs":[],"source":["visualize(X_train)"]},{"cell_type":"markdown","id":"61ab9370-4dd2-4243-969c-01ea63baeca4","metadata":{},"outputs":[],"source":["### Exercise 2 - Load CIFAR100 with coarse label_mode\n"]},{"cell_type":"markdown","id":"dc802368-ef33-40dd-8e49-cab8ed3c0d27","metadata":{},"outputs":[],"source":["Load the CIFAR100 dataset by setting the pramter ```label_mode = 'coarse'``` in the ```load_data``` function .\n"]},{"cell_type":"code","id":"af9500fa-b5f8-424c-8553-99eb968aad71","metadata":{},"outputs":[],"source":["# Write your solution here\n# Load the Fashion CIFAR100 dataset using the coarse label_mode\n\n"]},{"cell_type":"markdown","id":"25cb4a02-6362-402f-98c2-1a1ed9635ad5","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","\n","```python\n","(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data(label_mode = 'coarse')\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"44c785d0-b4f1-42c7-8b52-390433ca2d52","metadata":{},"outputs":[],"source":["Visualize a few images from the training set using the `visualize()` helper function.\n"]},{"cell_type":"code","id":"77216f78-3e24-4e98-9acc-9eda1f86f640","metadata":{},"outputs":[],"source":["# Write your solution here\n"]},{"cell_type":"markdown","id":"8de8b208-5392-4efc-b9ad-911eb1d14f0d","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","\n","```python\n","visualize(X_train)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"05833b32-decc-42d9-ad48-7dff31f85959","metadata":{},"outputs":[],"source":["### B. Load individual images as PIL objects\n"]},{"cell_type":"markdown","id":"78df4bd8-cb49-4895-9160-35fb81f07ea0","metadata":{},"outputs":[],"source":["We will save the following image into our working environment.\n"]},{"cell_type":"code","id":"1b42cf46-93e7-41a0-bdd4-65215c920f62","metadata":{},"outputs":[],"source":["import requests\n\nURL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/Dog_Breeds.jpg'\nfilename = URL.split('/')[-1]\nr = requests.get(URL, allow_redirects=True)\nopen(filename, 'wb').write(r.content)"]},{"cell_type":"markdown","id":"233219d0-d8dc-4c85-8ff2-baba61ef2d0d","metadata":{},"outputs":[],"source":["Next, we will specify a `target_size` of the image using a tuple of ints.\n"]},{"cell_type":"code","id":"8cd5e2bc-45b5-4e58-9162-4eb06e14e6c9","metadata":{},"outputs":[],"source":["img_height, img_width = 100, 100"]},{"cell_type":"markdown","id":"a62baef2-9227-4b28-ab64-f32b67676f34","metadata":{},"outputs":[],"source":["Using the path of the image we saved, we will use the `load_img` function to return a PIL Image instance.\n"]},{"cell_type":"code","id":"8270563c-f475-4f54-9821-7edde5877972","metadata":{},"outputs":[],"source":["gray_img = load_img(\n    'Dog_Breeds.jpg',\n    target_size=(img_height, img_width),\n    interpolation='nearest', grayscale = True)\n\ncolor_img = load_img(\n    'Dog_Breeds.jpg',\n    target_size=(img_height, img_width),\n    interpolation='nearest', grayscale = False)\n\n\nprint(type(gray_img))\nprint(type(color_img))"]},{"cell_type":"markdown","id":"72fd5856-f81a-411f-8d7b-258df34e1214","metadata":{},"outputs":[],"source":["We can plot each image \n"]},{"cell_type":"code","id":"ba1b1fe3-e45a-4fa1-ac68-b1d576f211c1","metadata":{},"outputs":[],"source":["plt.imshow(gray_img,cmap=\"gray\")\nplt.show()"]},{"cell_type":"code","id":"972a2b23-62a6-4e1b-ad3f-0bbb97838600","metadata":{},"outputs":[],"source":["plt.imshow(color_img)\nplt.show()"]},{"cell_type":"markdown","id":"e97cdc62-fec0-4b41-bef9-6c331ddabe75","metadata":{},"outputs":[],"source":["#### Converting an image\n"]},{"cell_type":"markdown","id":"9e37c466-50bb-4542-91ca-5eb831a00841","metadata":{},"outputs":[],"source":["We can convert the PIL image to a 3D numpy array of pixel data, and a single image to a batch using the following code:\n"]},{"cell_type":"code","id":"d6c79f8f-d869-46ec-a11d-c84feb24a119","metadata":{},"outputs":[],"source":["input_arr = tf.keras.preprocessing.image.img_to_array(color_img)\n\n\nprint(\"image shape\",input_arr.shape)\n"]},{"cell_type":"markdown","id":"31f834b7-0082-4fec-a043-2f34f8221709","metadata":{},"outputs":[],"source":["We need to add the batch dimension before the using  Keras, we can add the batch dimension as follows:\n"]},{"cell_type":"code","id":"68d1ff01-c66f-4a94-ae67-e608a0ff655d","metadata":{},"outputs":[],"source":["# Convert single image to a batch\ninput_arr_batch = np.array([input_arr])\n#or\ninput_arr_batch=input_arr.reshape(-1,input_arr.shape[0],input_arr.shape[1],input_arr.shape[2])"]},{"cell_type":"code","id":"d10e689c-b039-4529-91b9-3826825d5352","metadata":{},"outputs":[],"source":["print(\"image shape plus batch dimension\",input_arr_batch.shape)"]},{"cell_type":"markdown","id":"9ee4f2ed-912b-42c1-ae31-3781ee666adc","metadata":{},"outputs":[],"source":["Similarly, we can convert a numpy array of pixel data back to an image.\n"]},{"cell_type":"code","id":"4603ec24-5d93-4dc3-a354-0696368b9779","metadata":{},"outputs":[],"source":["# Write your solution here\n# Convert the numpy array back to an image\ncolor_img = tf.keras.preprocessing.image.array_to_img(input_arr)\nplt.imshow(color_img)\nplt.show()"]},{"cell_type":"markdown","id":"e9e0c6d8-66c5-444e-8f03-e741903dcd97","metadata":{},"outputs":[],"source":["#### Saving an image\n"]},{"cell_type":"code","id":"5b308285-df31-415c-94b6-4ed7da98e770","metadata":{},"outputs":[],"source":["tf.keras.preprocessing.image.save_img('dog_color_img.jpg', color_img)"]},{"cell_type":"markdown","id":"29f8bff4-6c3d-40ae-9748-4fe83a44d5ae","metadata":{},"outputs":[],"source":["### C. Load from directory of images\n"]},{"cell_type":"markdown","id":"4a149210-4078-4546-b1fc-cf51e9a6c134","metadata":{},"outputs":[],"source":["We can use the `ImageDataGenerator` class in Keras to load train, test, and validation datasets. This is especially useful when working with datasets containing several thousand or millions of images. More details on content provided in this sub-section can be found in the Tensorflow [documentation](https://www.tensorflow.org/tutorials/load_data/images?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01).\n"]},{"cell_type":"markdown","id":"1c2c43ba-9634-49b4-85fd-294f65dd6873","metadata":{},"outputs":[],"source":["We will first load in our dataset of flowers, and then look at what we have.\n","\n","The dataset has five types of flowers.\n","\n","| Category | Flower       |\n","| -------- | ------------ |\n","| 0        | `Daisies`    |\n","| 1        | `Dandelions` |\n","| 2        | `Roses`      |\n","| 3        | `Sunflowers` |\n","| 4        | `Tulips`     |\n","\n","Let's load the data and take a look.\n","\n","Because the flower dataset is not one of the default small toy datasets available, we'll need to download it using the `keras.utils.get_file()` function.  \n","Note that you will only need to specify the first three arguments.\n","\n","```python\n","keras.utils.get_file(\n","    fname=None,\n","    origin=None,\n","    untar=False,\n","    md5_hash=None,\n","    file_hash=None,\n","    cache_subdir='datasets',\n","    hash_algorithm='auto',\n","    extract=False,\n","    archive_format='auto',\n","    cache_dir=None\n",")\n","```\n"]},{"cell_type":"code","id":"ba48256b-ef27-4ab0-a06e-bffb5f02fadd","metadata":{},"outputs":[],"source":["import pathlib\ndataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/datasets/flower_photos.tgz\"\n\n# Download the data and track where it's saved using tf.keras.utils.get_file in a variable called data_dir\ndata_dir = keras.utils.get_file(origin=dataset_url,\n                                   fname='flower_photos',\n                                   untar=True)\ndata_dir = pathlib.Path(data_dir)\n\nfor folder in data_dir.glob('[!LICENSE]*'):\n    print('The', folder.name, 'folder has', len(list(folder.glob('*.jpg'))), 'pictures')\nimage_count = len(list(data_dir.glob('*/*.jpg')))\nprint(image_count, 'total images')"]},{"cell_type":"markdown","id":"8cb09920-a0a1-4e76-a5b1-0094831e48ba","metadata":{},"outputs":[],"source":["The files are stored in the following directory.\n","\n","```python\n","flower_photos\n","    └── daisy\n","    └── dandelion\n","    └── roses\n","    └── sunflowers\n","    └── tulips\n","\n","```\n"]},{"cell_type":"markdown","id":"6022a38a-3d60-4c52-9606-506e36554d23","metadata":{},"outputs":[],"source":["It seems there are a substantial amount of photos for each type of photo.\n","\n","We can checkout some of the images below.\n"]},{"cell_type":"markdown","id":"fba242f7-70cd-4903-9f4c-49eb4316172c","metadata":{},"outputs":[],"source":["#### Visualizing images\n"]},{"cell_type":"code","id":"705e608d-33dc-4056-9bb8-7cbefb359771","metadata":{},"outputs":[],"source":["dandelion = list(data_dir.glob('dandelion/*'))\nPIL.Image.open(str(dandelion[1]))"]},{"cell_type":"code","id":"7f23d0db-ddcb-4dbf-b0a0-27e13a6cc9c0","metadata":{},"outputs":[],"source":["roses = list(data_dir.glob('roses/*'))\nPIL.Image.open(str(roses[1]))"]},{"cell_type":"code","id":"377b82af-7419-4055-86c9-c1bba3b1c33e","metadata":{},"outputs":[],"source":["sunflowers = list(data_dir.glob('sunflowers/*'))\nPIL.Image.open(str(sunflowers[1]))"]},{"cell_type":"code","id":"fa37f369-5cf5-4705-a653-04824c4e465e","metadata":{},"outputs":[],"source":["daisy = list(data_dir.glob('daisy/*'))\n\nPIL.Image.open(str(daisy[1]))"]},{"cell_type":"markdown","id":"2b6f941e-840e-41a6-8671-0e7f07f9146d","metadata":{},"outputs":[],"source":["We can see that these photos are in various lighting conditions, framing, sizes, and zoom.\n"]},{"cell_type":"code","id":"cb5ab9f9-aa97-4c4c-98f3-302802128a52","metadata":{},"outputs":[],"source":["# The batch size simply specifies the number of images to pass through our neural network at a time, until the entire training set is passed through. 32 is the default\nbatch_size = 32\n\n# Here we set the size of all the images to be 200x200\nimg_height = 200\nimg_width = 200"]},{"cell_type":"markdown","id":"11aeda6b-941d-46ec-be41-5eec87d070e6","metadata":{},"outputs":[],"source":["Here we split our images into training and validation sets using the `keras.utils.image_dataset_from_directory()` function.\n","\n","*   Because our flower images are sorted into subfolders named after the type of flower, this method can automatically load in images and label them with the correct classes.\n","*   In order to plug these photos into our neural network as an input, we'll have to standardize the image sizes. We can do that with the `image_size` argument.\n"]},{"cell_type":"markdown","id":"6466df6f-f061-42ba-bf37-6afb79d92fe1","metadata":{},"outputs":[],"source":["Let us use `image_dataset_from_directory` to load images off disk:\n"]},{"cell_type":"code","id":"c00e1a36-f810-47d2-bd05-5bb4f31ad4f8","metadata":{},"outputs":[],"source":["train_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)"]},{"cell_type":"markdown","id":"af69ec7c-efe6-43ab-8559-9035ef56c9f1","metadata":{},"outputs":[],"source":["### Exercise 3 - Load validation set from the directory\n"]},{"cell_type":"markdown","id":"7300bd62-97f6-487e-a03f-c48e9ccab297","metadata":{},"outputs":[],"source":["Use the `image_dataset_from_directory` to load the validation set from the directory\n"]},{"cell_type":"code","id":"7e3e2ffa-020e-40dd-af76-f54c646bc1f5","metadata":{},"outputs":[],"source":["# Write your code here\n\n"]},{"cell_type":"markdown","id":"6b5e1d07-b9c4-4df1-a8f6-e17f42720734","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","\n","```python\n","  validation_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"ea5bb449-4944-4dd7-8d72-9bd0df01c9f3","metadata":{},"outputs":[],"source":["We can show the class names \n"]},{"cell_type":"code","id":"694b347e-fa53-45d2-8fd5-c4b20f01284c","metadata":{},"outputs":[],"source":["class_names = train_ds.class_names\nclass_names"]},{"cell_type":"markdown","id":"1f2bd9c5-dba9-4429-83fc-d688c2557c77","metadata":{},"outputs":[],"source":["Let's look at some sample rows from the dataset we loaded:\n"]},{"cell_type":"code","id":"53837bf7-1ab8-489f-b983-7c72de513695","metadata":{},"outputs":[],"source":["# .take() will take the first batch from a tensorflow dataset. \n# In our case it has taken the first 32 images.\nfirst_batch = train_ds.take(1)\n\nplt.figure(figsize = (25,10))\nfor img, lbl in first_batch:\n    # lets look at the first 10 images\n    for i in np.arange(10):\n        plt.subplot(2,5,i+1)\n        plt.imshow(img[i].numpy().astype('uint8'))\n        plt.title(class_names[lbl[i]])\n        plt.axis(\"off\")"]},{"cell_type":"markdown","id":"52957df7-1688-4a7f-a7ee-97d3bb957cbc","metadata":{},"outputs":[],"source":["We can see that after importing our images, they're all exactly square with `200 x 200` pixels.\n"]},{"cell_type":"markdown","id":"93823a5a-d84f-486d-8d78-77da95e04b01","metadata":{},"outputs":[],"source":["#### Augmenting Images\n"]},{"cell_type":"markdown","id":"9c58ca7c-e1aa-4594-abf1-3e7f83a4e5d5","metadata":{},"outputs":[],"source":["Next, we will see how we can use `ImageDataGenerator` class to create tensor image data batches that have been augmented to have certain characteristics. Image data augmentation is useful in expanding the training dataset in order to improve the performance and ability of the model to generalize. Here are some ways in which we can augment images in our dataset:\n"]},{"cell_type":"markdown","id":"eac31bd0-9bd6-4cc6-98c2-a2b08dbadceb","metadata":{},"outputs":[],"source":["*   **rescale**: rescaling factor\n","*   **rotation_range**: degree of random rotations of the images\n","*   **width_shift_range**: upper bound for random shift, either left or right\n","*   **height_shift_range**: upper bound for random shift, either up or down\n","*   **vertical_flip**: flip the image vertically\n"]},{"cell_type":"markdown","id":"90f128c5-af47-4385-b118-44c47217517b","metadata":{},"outputs":[],"source":["Further details on these options can be found in the Tensorflow [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01).\n"]},{"cell_type":"code","id":"e8448ce3-59f7-494e-9e05-0e2dec4add36","metadata":{},"outputs":[],"source":["import tensorflow as tf\n\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1.0 / 255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    vertical_flip=True,\n)\n\nflowers_data = img_gen.flow_from_directory(data_dir)\nimages, labels = next(flowers_data)\n\nprint(images.shape)\nprint(labels.shape)"]},{"cell_type":"markdown","id":"6412ae52-6b86-448e-bd6d-9e5ebab1a895","metadata":{},"outputs":[],"source":["### Exercise 4 - Visualizing augmented images\n"]},{"cell_type":"markdown","id":"60512154-7515-4786-ab29-671388292251","metadata":{},"outputs":[],"source":["Visualize 10 images from the augmented tensor image data batch.\n"]},{"cell_type":"code","id":"3a7dd41a-9625-4ace-81d9-2bed509f362d","metadata":{},"outputs":[],"source":["# Write your code here\n\n\n    "]},{"cell_type":"markdown","id":"434b6219-3bda-4df2-9606-554fd05cd6b0","metadata":{},"outputs":[],"source":["<details><summary>Solution</summary>\n","\n","```python\n","plt.figure(figsize=(25, 10))\n","for i in range(10):\n","    plt.subplot(2,5,i+1)\n","    plt.imshow(images[i])\n","    index = [index for index, each_item in enumerate(labels[i]) if each_item]\n","    plt.title(list(flowers_data.class_indices.keys())[index[0]])\n","    plt.axis(\"off\")\n","    \n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"26ffb512-dab6-4001-9f7e-641dc966ffca","metadata":{},"outputs":[],"source":["### D. Load image from URL\n"]},{"cell_type":"markdown","id":"891d144c-6633-4890-9a0f-e3bb3e5c17f5","metadata":{},"outputs":[],"source":["In the _Load individual images as PIL objects section_, we saw how we can download images to our working directory and, using the path of the image we saved, use the `load_img` function to return a PIL Image instance.\n"]},{"cell_type":"markdown","id":"0e62c612-cb10-4ab6-aa26-c29acae3b33d","metadata":{},"outputs":[],"source":["We can use custom functions like the one below to load images from a URL. This function was  borrowed from [statsmaths](https://statsmaths.github.io/stat289-f18/solutions/tutorial28-keras-images.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01).\n"]},{"cell_type":"code","id":"4d456e86-c279-4b7f-9702-3869c3229354","metadata":{},"outputs":[],"source":["def load_image(link, target_size=None):\n    import requests\n    import shutil\n    import os\n    _, ext = os.path.splitext(link)\n    r = requests.get(link, stream=True)\n    with open('temp.' + ext, 'wb') as f:\n        r.raw.decode_content = True\n        shutil.copyfileobj(r.raw, f)\n    img = tf.keras.preprocessing.image.load_img('temp.' + ext, target_size=target_size)\n    return tf.keras.preprocessing.image.img_to_array(img)"]},{"cell_type":"code","id":"d2946533-5b33-41db-878f-76ffdcec61bf","metadata":{},"outputs":[],"source":["URL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L1/Dog_Breeds.jpg'\n\nimg = load_image(URL, target_size=(100, 100))\nplt.imshow(img/255)"]},{"cell_type":"markdown","id":"8840f437-e46e-48c9-b949-76c50b25b18f","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"638e89ea-d8f5-4c24-89c8-4ee2e4714733","metadata":{},"outputs":[],"source":["[Kopal Garg](https://www.linkedin.com/in/gargkopal/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n"]},{"cell_type":"markdown","id":"9cfec500-4716-407f-a0e9-64833d4c17fb","metadata":{},"outputs":[],"source":["Kopal Garg is a Masters student in Computer Science at the University of Toronto.\n"]},{"cell_type":"markdown","id":"32415463-01d4-4de1-a717-f0d5a3e03f0d","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"4721eb8b-ad37-4c29-85e7-f7c647fcf262","metadata":{},"outputs":[],"source":["[Richard Ye](https://linkedin.com/in/richard-ye?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01), [Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n"]},{"cell_type":"markdown","id":"94ffb0f4-4f13-4a72-8025-34abe17632fb","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"552823fc-5e54-4576-a463-0a601571d4fb","metadata":{},"outputs":[],"source":["| Date (YYYY-MM-DD) | Version | Changed By  | Change Description |\n","| ----------------- | ------- | ----------- | ------------------ |\n","| 2022-05-23        | 0.1     | Kopal Garg  | Created Lab        |\n","| 2022-05-23        | 0.1     | Richard Ye  | Load from directory of images        |\n","| 2022-05-30        | 0.1     | Roxanne Li  | Review and edit content|\n","| 2022-07-19        | 0.1     | Steve Hord  | QA pass |\n"]},{"cell_type":"markdown","id":"3522da21-6c08-4f4a-b819-fd56d819ad94","metadata":{},"outputs":[],"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"f883a44d4354b7f6d605286efd229cf3c45bdc0c48451e1eb874db81463376f3"},"nbformat":4,"nbformat_minor":4}