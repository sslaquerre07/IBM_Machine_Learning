{"cells":[{"cell_type":"markdown","id":"e9311a7e-d3db-4134-bfe7-70ff84dc258d","metadata":{},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n","</center>\n"]},{"cell_type":"markdown","id":"02e33d7e-a761-4b78-b83c-2f76a37b8a50","metadata":{},"source":["# **End-to-End Example: Transfer Learning**\n"]},{"cell_type":"markdown","id":"d77fbc83-c896-4a12-b364-0e0f515282cc","metadata":{},"source":["Estimated time needed: **45** minutes\n"]},{"cell_type":"markdown","id":"29acbeb5-a0f4-4405-a3a5-0c6c0e31da30","metadata":{},"source":["In this lab, we will walk through an end-to-end example of transfer learning and fine-tuning. \n"]},{"cell_type":"markdown","id":"1fd771a9-c345-457b-9818-d35038e17d2c","metadata":{},"source":["<h1> Is the waste product organic or a recyclable? </h1></s>\n"]},{"cell_type":"markdown","id":"6955b3cc-d707-4322-99ba-c6da0dd9f983","metadata":{},"source":["You are a data science intern at a waste management service. Your manager has asked you to create a waste classification pipeline that categorizes waste streams based on disposal options: organic or recyclable. \n","\n","In this lab we're going to train a transfer learning model to perform this image classification task.\n"]},{"cell_type":"markdown","id":"5faf8e5a-0ce4-4e6b-91f6-076ee53d0b75","metadata":{},"source":["<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L2/img/transfer_learning.gif\" width=\"600\" height=\"600\">\n"]},{"cell_type":"markdown","id":"a66f790e-34bd-46a1-a2a1-c5a1efcb6a75","metadata":{},"source":["## **Table of Contents**\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","    </li>\n","    <li>\n","        <a href=\"#Background\">Background</a>\n","        <ol>\n","            <li><a href=\"#What is transfer learning?\">What is transfer learning?</a></li>\n","        </ol>\n","    </li>\n","        <li>\n","        <a href=\"#Example 1: Create a ML model for distinguishing recyclable and organic waste images\">Example 1: Create a ML model for distinguishing recyclable and organic waste images</a>\n","            <ol>\n","            <li><a href=\"#Dataset\">Dataset</a></li>\n","            <li><a href=\"#Importing data\">Importing data</a></li>\n","            <li><a href=\"#Data visualization\">Data visualization</a></li>\n","            <li><a href=\"#Define configuration options\">Define configuration options</a></li>\n","            <li><a href=\"#Loading Images using ImageGeneratorClass\">Loading Images using ImageGeneratorClass</a></li>\n","            <li><a href=\"#Pre-trained models\">Pre-trained models</a></li>\n","            <li><a href=\"#Compile the model\">Compile the model</a></li>\n","            <li><a href=\"#Fine-Tuning\">Fine-Tuning</a></li>\n","            <li><a href=\"#Evaluate both models on test data\">Evaluate both models on test data</a></li>\n","        </ol>\n","        </li>\n","    <li>\n","        <a href=\"#Example 2: Use Transfer Learning for identifying Stop Signs\">Example 2: Use Transfer Learning for identifying Stop Signs</a>\n","            <ol>\n","            <li><a href=\"#Loading images\">Loading images</a></li>\n","            <li><a href=\"#Defining a helper function\">Defining a helper function</a></li>\n","            <li><a href=\"#Pre-trained Model 1: Incepton-v3\">Pre-trained Model 1: Incepton-v3</a></li>\n","            <li><a href=\"#Pre-trained Model 2: MobileNet\">Pre-trained Model 2: MobileNet</a></li>\n","            <li><a href=\"#Pre-trained Model 3: ResNet-50\">Pre-trained Model 3: ResNet-50</a></li>\n","         </ol>\n","        </li>\n","</ol>\n"]},{"cell_type":"markdown","id":"345b8359-a497-4e38-8c3e-d76c59f3cc66","metadata":{},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","- __Perform__ pre-processing and image augmentation on ImageGeneratorClass objects in Keras. \n","- __Implement__ transfer learning in five general steps: \n","    - obtain pre-trained model, \n","    - create base model, \n","    - freeze layers, \n","    - train new layers on dataset, \n","    - improve model through fine tuning.\n","- __Build__ an end-to-end VGG16-based transfer learning model for binary image classification tasks.\n"]},{"cell_type":"markdown","id":"dc2a058d-a767-4d25-ac99-71451e29c152","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","id":"fe9828ce-7ca4-4790-9c14-df8df09968cb","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n","*   [`keras`](https://keras.io/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for loading datasets.\n"]},{"cell_type":"markdown","id":"21f326c0-ab5b-4ea7-8287-e96ff7f86876","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip install tensorflow_datasets`, `!pip install --upgrade tensorflow` and `!pip install opendatasets` in the following code cell.\n"]},{"cell_type":"code","execution_count":1,"id":"57133bbb-47e0-4974-a368-1220ad2b69b0","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"code","execution_count":2,"id":"5b36e504-d871-4aa7-926d-c9ef41270667","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (2.17.0)\n","Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow) (2.17.0)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (73.0.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.5)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n","Requirement already satisfied: rich in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in c:\\github\\ibm_machine_learning\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"]}],"source":["!pip install --upgrade tensorflow"]},{"cell_type":"markdown","id":"45a6af7d-f78f-43c1-9163-a0357e544253","metadata":{},"source":["### Importing Required Libraries\n"]},{"cell_type":"code","execution_count":3,"id":"7072bb61-99d3-4e0c-9bfc-cfa6c0b59069","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.18.0-dev20240815\n"]}],"source":["import numpy as np\n","import datetime\n","import os\n","import random, shutil\n","import glob\n","import skillsnetwork\n","\n","import warnings\n","warnings.simplefilter('ignore')\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n","\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from matplotlib import pyplot\n","from matplotlib.image import imread\n","\n","from os import makedirs,listdir\n","from shutil import copyfile\n","from random import seed\n","from random import random\n","import keras \n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow.keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n","from keras.models import Sequential\n","from keras import optimizers\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Input\n","from tensorflow.keras.layers import Dense, Dropout, Flatten\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import InceptionV3\n","from sklearn import metrics\n","\n","\n","sns.set_context('notebook')\n","sns.set_style('white')"]},{"cell_type":"markdown","id":"d14d0ce9-f77c-4bf0-98ee-647217bcb1e1","metadata":{},"source":["## Background\n"]},{"cell_type":"markdown","id":"cd2b5d1b-1434-4885-a1f5-5d929408148b","metadata":{},"source":["### What is transfer learning?\n"]},{"cell_type":"markdown","id":"703c4dd0-6a91-46df-9dab-8e8e81ad233d","metadata":{},"source":["Most popular models are difficult to train from scratch as they require huge datasets (like ImageNet), a large number of training iterations, and very heavy computing machinery. The basic features (edges, shapes) learned by early layers in a network are generalizable. While the later layers in an already trained network tend to capture features that are more particular to a specific image classification task. \n","\n","__Transfer learning__ uses the idea that if we keep the early layers of a pre-trained network, and re-train the later layers on a specific dataset, we might be able to leverage some state of that network on a related task.\n"]},{"cell_type":"markdown","id":"50ab74a4-9b39-4de8-90c5-51c93975f6bd","metadata":{},"source":["A typical transfer learning workflow in Keras looks something like this:\n","    \n","1. Initialize base model, and load pre-trained weights (like ImageNet).\n","2. \"Freeze\" layers in the base model by setting `training = False`.\n","3. Define a new model that goes on top of the output of the base model's layers.\n","4. Train resulting model on your data set.\n"]},{"cell_type":"markdown","id":"49b5031a-b23c-4e9b-a4e7-5aa204b56add","metadata":{},"source":["## Example 1: Create a ML model for distinguishing recyclable and organic waste images\n"]},{"cell_type":"markdown","id":"448e55b0-b3bf-42a8-a232-ea55287e9686","metadata":{},"source":["### Dataset\n"]},{"cell_type":"markdown","id":"df94c620-0b21-46aa-996d-f0f0d5253837","metadata":{},"source":["The dataset used in this Lab was found on Kaggle, which is an awesome community for people who enjoy anything related to data science and machine learning. We will be using the Waste Classification Dataset [dataset](https://www.kaggle.com/datasets/techsash/waste-classification-data):\n"]},{"cell_type":"markdown","id":"e8742ccd-5c5d-484e-a433-f97f936a85c9","metadata":{},"source":["> PROBLEM: \n","> - Waste management is a big problem in our country. Most of the wastes end up in landfills.\n","\n","> APPROACH: \n","> - Segregated into two classes (Organic and recyclable)\n","> - Automated the process by using IOT and machine learning\n","\n","> IMPLEMENTATION:\n","> - Dataset is divided into train data (85%) and test data (15%)\n","> - Training data - 22,564 images Test data - 2,513 images\n"]},{"cell_type":"markdown","id":"e37a98dc-a9e8-45c4-ae88-dedff886061e","metadata":{},"source":["* The dataset contains ~25,000 images of recyclable and organic products split into a train and test set.\n","* Our goal is to train an algorithm on these files and to predict the labels for images in our test set (1 = recyclable, 0 = organic).\n"]},{"cell_type":"markdown","id":"f85ee6f6-2c12-4d7a-8470-b6a774e70ec1","metadata":{},"source":["Now that we have obtained the necessary context and some insight in today's dataset, we're moving on to the practical part: using transfer learning to classify images. In this task, we will be using Tensorflow, which is a highly used ML library for training neural networks, and Keras, which is an API that makes this process simpler.\n"]},{"cell_type":"markdown","id":"cea32fea-8822-4cf6-873f-8fd940656ba6","metadata":{},"source":["We will go through the following steps to create our classification model:\n"]},{"cell_type":"markdown","id":"b098bfa3-6b26-49a8-bedb-2aef1fc45c0e","metadata":{},"source":["1. Import data directly from Kaggle.\n","2. Visualize a few random images from the train set.\n","3. Load images in using the ImageGeneratorClass from Keras.\n","4. Define model configuration options.\n","5. Perform some image augmentation for improved model generalizability.\n","6. Load, compile and train a pre-trained model like VGG-16.\n","7. Perform inference on the test set. \n"]},{"cell_type":"markdown","id":"a39954f1-b3a2-4a69-bfa4-16bba73d5949","metadata":{},"source":["### Importing Data\n"]},{"cell_type":"markdown","id":"e00b9576-252b-4de5-a860-e140a1f226d3","metadata":{},"source":["This will create a `o-vs-r-split` directory in your environment.\n"]},{"cell_type":"markdown","id":"24559647-5fb4-458e-a5be-dc06d8ef4261","metadata":{},"source":["### Define configuration options\n"]},{"cell_type":"markdown","id":"9c4adcf7-4482-4c21-845e-5993b506e212","metadata":{},"source":["It's time to define some model configuration options.\n"]},{"cell_type":"markdown","id":"e93e4c11-063a-4512-bd36-9e8215dfd170","metadata":{},"source":["* __img_rows__ and __img_cols__ are used to specify width and height of the images expected by MobileNet.\n","* __batch size__ is set to 32.\n","* The __number of epochs__ (that is, iterations) is set to 5, but as we are using Early Stopping, the number of iterations might be lower if early stopping conditions are met before 5 iterations.\n","* The __number of classes__ is 2.\n","* We will use 20% of the data for __validation__ purposes.\n","* We want to be able to see the model output, so we set __verbosity__ to 1, or True.\n","* The __path__ is a path to the directory with the training data and __path_test__ should contain the test data.\n","* The __input_shape__ is common for an image: (w, h, d).\n","* We have two __labels__ in our dataset: organic (O), recyclable (R).\n","* The __checkpoint_path__ is where ModelCheckpoint will save our model.\n"]},{"cell_type":"code","execution_count":18,"id":"481b478a-c710-4582-92c4-2619853acdb4","metadata":{},"outputs":[],"source":["img_rows, img_cols = 150, 150\n","batch_size = 32\n","n_epochs = 10\n","n_classes = 2\n","val_split = 0.2\n","verbosity = 1\n","path = 'o-vs-r-split/train/'\n","path_test = 'o-vs-r-split/test/'\n","input_shape = (img_rows, img_cols, 3) #RGB\n","labels = ['O', 'R']\n","seed = 10\n","checkpoint_path='ORnet.h5'"]},{"cell_type":"markdown","id":"ec4591a7-fb20-40cd-83db-ab52036f1e5d","metadata":{},"source":["### Loading Images using ImageGeneratorClass\n"]},{"cell_type":"markdown","id":"027b9885-f28e-4a50-b6c3-dd01f3daae17","metadata":{},"source":["Transfer learning works best when models are trained on smaller datasets. We are using 10,000 images from the original dataset of 25,000 images. We are setting aside 10% of the set for testing purposes.\n"]},{"cell_type":"markdown","id":"a7a27de2-1e4a-4d9f-bcb8-113c21d56162","metadata":{},"source":["The folder structure looks as follows:\n"]},{"cell_type":"markdown","id":"d90e270b-a679-4b3f-acc4-df0eb25b25e0","metadata":{},"source":["```python\n","o-vs-r-split/\n","└── train\n","    ├── O\n","    └── R\n","└── test\n","    ├── O\n","    └── R\n","```\n"]},{"cell_type":"markdown","id":"4cd10076-1ce4-4d5a-9447-0452ed10fc34","metadata":{},"source":["#### Image Augmentation\n"]},{"cell_type":"markdown","id":"855377a9-77f1-4904-b1b1-47b611d5ccdf","metadata":{},"source":["Now we will create ImageDataGenerators used for training, validation, and testing.\n","\n","Image data generators create batches of tensor image data with real-time data augmentation. They loop over the data in batches and are useful in feeding data to the training process. We specify a 20% validation split.\n","\n","We will perform a few image augmentation steps. We set __rescale__ to 1./255. This means that each image pixel will be divided by 255 in order to normalize the image. We set the __width_shift_range__ and __height_shift_range__ to 0.1 each, and the __horizontal_flip__ to True. \n","* The __width_shift_range__ shifts the image horizontally (left or right). \n","* The __height_shift_range__ shifts the image vertically (up or down). \n","* The __horizontal_flip__ randomly flip inputs horizontally.\n"]},{"cell_type":"code","execution_count":5,"id":"c7748fb0-c904-482f-b31b-8bcd64502f8b","metadata":{},"outputs":[],"source":["# Create ImageDataGenerators for training and validation and testing\n","train_datagen = ImageDataGenerator(\n","    validation_split = val_split,\n","    rescale=1.0/255.0,\n","\twidth_shift_range=0.1, \n","    height_shift_range=0.1, \n","    horizontal_flip=True\n",")\n","\n","val_datagen = ImageDataGenerator(\n","    validation_split = val_split,\n","    rescale=1.0/255.0,\n","\twidth_shift_range=0.1, \n","    height_shift_range=0.1, \n","    horizontal_flip=True\n",")\n","\n","test_datagen = ImageDataGenerator(\n","    rescale=1.0/255.0\n",")"]},{"cell_type":"markdown","id":"658cd779-6b53-40de-8188-da77aec1d4ba","metadata":{},"source":["Since it is undesirable to load all of our image data into memory all at once, we make use of the __flow_from_directory__ method. It takes the ImageDataGenerator training/validation sets and flows image batches from a specified folder. \n","\n","* We set __directory__ to the __path__, specifying the path to our training dataset. \n","* We specify classes using __labels__ we had defined in the configuration step.\n","* We apply a __seed__ so our random initializer is generated in the same way each time we perform this step. This allows for direct comparison between experiments.\n"]},{"cell_type":"code","execution_count":6,"id":"b86fe7d6-e773-4b5a-bad4-d66ac297ead6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5810 images belonging to 2 classes.\n"]}],"source":["# use the labels defined before to \n","# find number of images belonging to each category\n","train_generator = train_datagen.flow_from_directory(\n","    directory = path,\n","    classes = labels,\n","    seed = seed,\n","    batch_size = batch_size, \n","    class_mode='binary',\n","    shuffle = True,\n","    target_size=(img_rows, img_rows),\n","    subset = 'training'\n",")"]},{"cell_type":"code","execution_count":7,"id":"61ea77cc-32ad-4d23-a5ba-68170aba34db","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1452 images belonging to 2 classes.\n"]}],"source":["val_generator = val_datagen.flow_from_directory(\n","    directory = path,\n","    classes = labels,\n","    seed = seed,\n","    batch_size = batch_size, \n","    class_mode='binary',\n","    shuffle = True,\n","    target_size=(img_rows, img_rows),\n","    subset = 'validation'\n",")"]},{"cell_type":"code","execution_count":7,"id":"160fc238-38a1-4b70-98ef-d1a9f0cb213f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 828 images belonging to 2 classes.\n"]}],"source":["test_generator = test_datagen.flow_from_directory(\n","    directory = path_test,\n","    classes = labels,\n","    class_mode='binary',\n","    seed = seed,\n","    batch_size = batch_size, \n","    shuffle = True,\n","    target_size=(img_rows, img_rows)\n",")"]},{"cell_type":"markdown","id":"631d3c85-a3cb-4462-87c2-87022261d201","metadata":{},"source":["There were 5810 images belonging to 2 classes found in the train directory, and 1452 images belonging to the same two classes were found in the validation directory, and 828 images were found in the test set.\n"]},{"cell_type":"markdown","id":"d615c41a-4bc4-4ff4-af5a-023616773708","metadata":{},"source":["Let's look at a few augmented images:\n"]},{"cell_type":"code","execution_count":8,"id":"b5ef2d97-0e67-4efe-a1c8-8f2fb2feec89","metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["IMG_DIM = (100, 100)\n","\n","train_files = glob.glob('./o-vs-r-split/train/O/*')\n","train_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=IMG_DIM)) for img in train_files]\n","train_imgs = np.array(train_imgs)\n","train_labels = [fn.split('/')[3].split('.')[0].strip() for fn in train_files]\n","\n","img_id = 0\n","O_generator = train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],\n","                                   batch_size=1)\n","O = [next(O_generator) for i in range(0,5)]\n","fig, ax = plt.subplots(1,5, figsize=(16, 6))\n","print('Labels:', [item[1][0] for item in O])\n","l = [ax[i].imshow(O[i][0][0]) for i in range(0,5)]\n"]},{"cell_type":"markdown","id":"b9b8f6ca-a867-4c93-b7c6-b7d3c9fe7f18","metadata":{},"source":["### Pre-trained Models\n"]},{"cell_type":"markdown","id":"708d7078-8d9a-49eb-a323-800175f7c894","metadata":{},"source":["Pre-trained models are saved networks that have previously been trained on some large (somewhat related) datasets. They are typically used for a large-scale image-classification task. They can be used as they are or could be customized to a given task using transfer learning. These pre-trained models form the basis of transfer learning.\n"]},{"cell_type":"markdown","id":"3a5d0b8b-3059-4018-88eb-fee422815ac4","metadata":{},"source":["We can leverage a pre-trained model's weighted layers to extract generic features. In this step we would not update the model's layers weights during training. This helps us utilize the knowledge from a source-domain task.\n"]},{"cell_type":"markdown","id":"af78b072-f6c5-4cf9-8d9b-d53bf4f8206c","metadata":{},"source":["Fine-tuning, which is a more involved process, involves more than just replacing the final layer. We also try to retrain some of the previous layers.\n"]},{"cell_type":"markdown","id":"72e4a482-6bcf-4f68-8fa0-7681cbf26c92","metadata":{},"source":["In computer vision, some popular pre-trained models include: VGG-16, VGG-19, InceptionV3, XCeption, and ResNet-50.\n"]},{"cell_type":"markdown","id":"9ae69bc5-fcd1-42d5-b0fa-f71e567d4a65","metadata":{},"source":["#### VGG-16\n"]},{"cell_type":"markdown","id":"603f24b3-7451-4f63-9f07-35c9dc908186","metadata":{},"source":["Let us load the VGG16 model.\n","\n","```python\n","tf.keras.applications.VGG16(\n","    include_top=True,\n","    weights=\"imagenet\",\n","    input_shape=None,\n",")\n","```\n","The default input image size for this model is 224x224.\n","\n","Further information on arguments can be found in the Keras [documentation](https://keras.io/api/applications/vgg/).\n","\n",">Note: Each Keras Application expects a specific kind of input preprocessing. For VGG16, call `tf.keras.applications.vgg16.preprocess_input` on your inputs before passing them to the `model.vgg16.preprocess_input` will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n","\n",">* __include_top__: whether to include the fully-connected layer at the top of the network.\n",">* __weights__: one of None (random initialization), 'imagenet' (pre-trained on ImageNet), or the path to the weights file to be loaded.\n",">* __input_shape__: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3). \n"]},{"cell_type":"markdown","id":"1015a375-6a64-45f2-9b78-09dbbdfbc144","metadata":{},"source":["First we load the model \n"]},{"cell_type":"code","execution_count":8,"id":"eb36424d-c828-4fa5-8776-d14a36746fc3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n"]}],"source":["from keras.applications import vgg16\n","input_shape = (150, 150, 3)\n","\n","vgg = vgg16.VGG16(include_top=False,\n","                        weights='imagenet',\n","                        input_shape=input_shape)\n","\n"]},{"cell_type":"markdown","id":"d9e10a23-d9b8-4ab0-8093-e7a480b67115","metadata":{},"source":["We flatten the output of a vgg model and assign it to the model ```output```, we then use a Model object ```basemodel``` to group the layers into an object for training and inference .\n","With the following inputs and outputs  \n","\n","inputs: ```vgg.input```\n","\n","outputs: ```tf.keras.layers.Flatten()(output)```\n"]},{"cell_type":"code","execution_count":9,"id":"eaae250a-030f-4cdf-aa5e-57f2e6357b97","metadata":{},"outputs":[],"source":["output = vgg.layers[-1].output\n","output = tf.keras.layers.Flatten()(output)\n","basemodel = Model(vgg.input, output)"]},{"cell_type":"markdown","id":"ebb7cc2d-c675-4422-a8ea-4bbf9bd31581","metadata":{},"source":["Next, we freeze the basemodel, like the lower layers.\n"]},{"cell_type":"code","execution_count":10,"id":"3564cf8e-d881-4f2e-a3c8-226014fa584c","metadata":{},"outputs":[],"source":["basemodel.trainable = False\n","for layer in basemodel.layers: layer.trainable = False"]},{"cell_type":"markdown","id":"1d75bcf9-5760-4d10-a21f-f1568804dda8","metadata":{},"source":["Create a new model on top. We add a Dropout layer for regularization, only these layers will change as for the lower layers we set `training=False` when calling the base model.\n"]},{"cell_type":"code","execution_count":11,"id":"9382185a-3471-4cf9-9ed4-d2b66e1b3351","metadata":{},"outputs":[],"source":["input_shape = basemodel.output_shape[1]\n","\n","model = Sequential()\n","model.add(basemodel)\n","model.add(Dense(512, activation='relu', input_dim=input_shape))\n","model.add(Dropout(0.3))\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(1, activation='sigmoid'))"]},{"cell_type":"markdown","id":"404628ac-4173-42a1-8ba2-0cbad1046d0b","metadata":{},"source":["Let us print the model summary.\n"]},{"cell_type":"code","execution_count":12,"id":"3d6e6983-e010-4025-9e4d-cc55c0993e64","metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ functional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,816</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ functional (\u001b[38;5;33mFunctional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │    \u001b[38;5;34m14,714,688\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m4,194,816\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,172,673</span> (73.14 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,172,673\u001b[0m (73.14 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,457,985</span> (17.01 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,457,985\u001b[0m (17.01 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["None\n"]}],"source":["print(model.summary())"]},{"cell_type":"markdown","id":"ffa3f4eb-d24f-4d6a-bf05-c5448bb14470","metadata":{},"source":["### Compile the model\n"]},{"cell_type":"markdown","id":"f61877bf-082e-4c4c-bf87-8c20396f1331","metadata":{},"source":["Calling `compile()` freezes the behavior of that model, implying that the trainable attribute values at the time of compilation are preserved.\n"]},{"cell_type":"markdown","id":"7b2a6939-49bb-48c4-9ffb-ba7194f2811b","metadata":{},"source":["We use RMSProp as our optimizer with a learning rate of 2e-5, Binary Cross Entropy Loss as our loss function and Accuracy as our primary metric for model evaluation as we have 2 labels in our dataset.\n"]},{"cell_type":"code","execution_count":13,"id":"330fba30-c2eb-4141-b460-53982049f3ca","metadata":{},"outputs":[],"source":["model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(learning_rate=2e-5),\n","              metrics=['accuracy'])\n"]},{"cell_type":"markdown","id":"1e646a2d-649c-47e6-9032-6315959c99a9","metadata":{},"source":["We use early stopping to avoid over-training the model. An over-trained model overfits the training dataset and has poor performance on unseen test sets. We will also use a exponential step-decay based learning rate scheduler.\n"]},{"cell_type":"code","execution_count":19,"id":"132323de-4453-4c79-9b33-da46a2f52331","metadata":{},"outputs":[],"source":["from keras.callbacks import LearningRateScheduler\n","checkpoint_path='O_R_tlearn_image_augm_cnn_vgg16.weights.h5'\n","\n","# define step decay function\n","class LossHistory_(keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","        self.lr = []\n","        \n","    def on_epoch_end(self, batch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","        self.lr.append(exp_decay(len(self.losses)))\n","        print('lr:', exp_decay(len(self.losses)))\n","\n","def exp_decay(epoch):\n","    initial_lrate = 1e-5\n","    k = 0.1\n","    lrate = initial_lrate * np.exp(-k*epoch)\n","    return lrate\n","\n","# learning schedule callback\n","loss_history_ = LossHistory_()\n","lrate_ = LearningRateScheduler(exp_decay)\n","\n","keras_callbacks = [\n","      EarlyStopping(monitor = 'loss', \n","                    patience = 5, \n","                    mode = 'min', \n","                    min_delta=0.01),\n","      ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, mode='min', save_weights_only=True)\n","]\n","\n","callbacks_list_ = [loss_history_, lrate_, keras_callbacks]"]},{"cell_type":"markdown","id":"cc035317-b6b2-4777-8377-1a176c20fb79","metadata":{},"source":["### Fit and train the model\n"]},{"cell_type":"code","execution_count":20,"id":"fe1861f1-2c40-4fa8-a555-c4483fd2ffef","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.5273 - loss: 0.7114lr: 9.048374180359596e-06\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 15s/step - accuracy: 0.5268 - loss: 0.7113 - val_accuracy: 0.6781 - val_loss: 0.6309 - learning_rate: 1.0000e-05\n","Epoch 2/5\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.5820 - loss: 0.6683lr: 8.18730753077982e-06\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 11s/step - accuracy: 0.5811 - loss: 0.6685 - val_accuracy: 0.6969 - val_loss: 0.6187 - learning_rate: 9.0484e-06\n","Epoch 3/5\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5696 - loss: 0.6798lr: 7.408182206817179e-06\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 8s/step - accuracy: 0.5741 - loss: 0.6768 - val_accuracy: 0.6375 - val_loss: 0.6104 - learning_rate: 8.1873e-06\n","Epoch 4/5\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6747 - loss: 0.5960lr: 6.703200460356394e-06\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 7s/step - accuracy: 0.6741 - loss: 0.5961 - val_accuracy: 0.7344 - val_loss: 0.5637 - learning_rate: 7.4082e-06\n","Epoch 5/5\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31s/step - accuracy: 0.6309 - loss: 0.6199 lr: 6.065306597126334e-06\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 32s/step - accuracy: 0.6315 - loss: 0.6184 - val_accuracy: 0.7209 - val_loss: 0.5718 - learning_rate: 6.7032e-06\n"]}],"source":["extract_feat_model = model.fit(train_generator, \n","                              steps_per_epoch=10, \n","                              epochs=5,\n","                              validation_data=val_generator, \n","                              validation_steps=10, \n","                              verbose=1,\n","                              callbacks = callbacks_list_)  "]},{"cell_type":"markdown","id":"5660ae9a-6440-4cea-98c8-afa630933049","metadata":{},"source":["### Fine-Tuning\n"]},{"cell_type":"markdown","id":"90455f01-5fc2-4c04-962e-963e6eb890b1","metadata":{},"source":["Fine-tuning is an optional step in transfer learning, it usually ends up improving the performance of the model. It is easy to overfit the model in this step as we are re-training the entire model. So we use regularization (dropout layers), a lower learning rate, a small number of epochs (training iterations), and early stopping to know when the model has stopped improving and to prevent overfitting. \n"]},{"cell_type":"markdown","id":"acd94bd0-b910-46c1-9e5d-67ac15bd33fa","metadata":{},"source":["In order to fine-tune our model, we will use the VGG-16 model object stored in the `basemodel` variable. We will unfreeze convolution blocks 4 and 5 and keep the first 3 blocks frozen. This ensures that the convolution and pooling layers for blocks 4 and 5 are trainable; that is, weights get updated through backpropagation in each training iteration or epoch.\n"]},{"cell_type":"markdown","id":"18e467a6-cc38-4e81-8202-ecb89961e73e","metadata":{},"source":["We can find the name of each layer\n"]},{"cell_type":"code","execution_count":21,"id":"0aeece04-b9f5-4255-b3ad-7ac4b55e8853","metadata":{},"outputs":[{"data":{"text/plain":["['input_layer',\n"," 'block1_conv1',\n"," 'block1_conv2',\n"," 'block1_pool',\n"," 'block2_conv1',\n"," 'block2_conv2',\n"," 'block2_pool',\n"," 'block3_conv1',\n"," 'block3_conv2',\n"," 'block3_conv3',\n"," 'block3_pool',\n"," 'block4_conv1',\n"," 'block4_conv2',\n"," 'block4_conv3',\n"," 'block4_pool',\n"," 'block5_conv1',\n"," 'block5_conv2',\n"," 'block5_conv3',\n"," 'block5_pool',\n"," 'flatten']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["[layer.name for layer in basemodel.layers]"]},{"cell_type":"markdown","id":"2b7dd5c7-8504-4f61-b3c8-f20233cc1b26","metadata":{},"source":["we then set ```'block5_conv1'```and ```'block4_conv1' ``` to trainable\n"]},{"cell_type":"code","execution_count":22,"id":"a3e54a34-e8a4-4dba-afaa-f4e86eace22d","metadata":{},"outputs":[],"source":["basemodel.trainable = True\n","\n","set_trainable = False\n","\n","for layer in basemodel.layers:\n","    if layer.name in ['block5_conv1', 'block4_conv1']:\n","        set_trainable = True\n","    if set_trainable:\n","        layer.trainable = True\n","    else:\n","        layer.trainable = False"]},{"cell_type":"markdown","id":"36cffaaf-257c-478d-8085-5e997e7ca736","metadata":{},"source":["Similar to what we did before, we create a new model on top, add a Dropout layer for regularization, and set `training=False` when calling the base model.\n"]},{"cell_type":"code","execution_count":null,"id":"bac643fc-19d4-492c-a063-33661aae327d","metadata":{},"outputs":[],"source":["model = Sequential()\n","model.add(basemodel)\n","model.add(Dense(512, activation='relu', input_dim=input_shape))\n","model.add(Dropout(0.3))\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","checkpoint_path='O_R_tlearn_image_augm_fine_tune_vgg16.h5'\n","\n","\n","# learning schedule callback\n","loss_history_ = LossHistory_()\n","lrate_ = LearningRateScheduler(exp_decay)\n","\n","keras_callbacks = [\n","      EarlyStopping(monitor = 'loss', \n","                    patience = 5, \n","                    mode = 'min', \n","                    min_delta=0.01),\n","      ModelCheckpoint(checkpoint_path, monitor='loss', save_best_only=True, mode='min')\n","]\n","\n","callbacks_list_ = [loss_history_, lrate_, keras_callbacks]\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(learning_rate=1e-5),\n","              metrics=['accuracy'])\n","              \n","fine_tune_model = model.fit(train_generator, \n","                    steps_per_epoch=10, \n","                    epochs=5,\n","                    callbacks = callbacks_list_,   \n","                    validation_data=val_generator, \n","                    validation_steps=10, \n","                    verbose=1)       "]},{"cell_type":"markdown","id":"fe937f0b-03eb-40c3-922f-2051a46e566e","metadata":{},"source":["The fine tuned model has a validation accuracy higher than the previous transfer learning model.\n"]},{"cell_type":"markdown","id":"64244880-297b-46b5-8bb9-f0d548125513","metadata":{},"source":["### Evaluate both models on test data\n"]},{"cell_type":"markdown","id":"f7345e01-be92-4e29-ba83-284937e7aa83","metadata":{},"source":["Load saved models:\n"]},{"cell_type":"code","execution_count":24,"id":"4913dddc-3bdf-4a86-aa48-8305af8e1c71","metadata":{},"outputs":[{"ename":"ValueError","evalue":"No model config found in the file at O_R_tlearn_image_augm_cnn_vgg16.weights.h5.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m extract_feat_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mO_R_tlearn_image_augm_cnn_vgg16.weights.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\GitHub\\IBM_Machine_Learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:194\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    188\u001b[0m         filepath,\n\u001b[0;32m    189\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    191\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n","File \u001b[1;32mc:\\GitHub\\IBM_Machine_Learning\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:125\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    123\u001b[0m model_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model config found in the file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    129\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mValueError\u001b[0m: No model config found in the file at O_R_tlearn_image_augm_cnn_vgg16.weights.h5."]}],"source":["extract_feat_model = tf.keras.models.load_model('O_R_tlearn_image_augm_cnn_vgg16.weights.h5')"]},{"cell_type":"markdown","id":"0b43edad-c630-4183-b6e2-21ed2f8d7a1c","metadata":{},"source":["Load test images:\n"]},{"cell_type":"code","execution_count":null,"id":"5b984905-4d60-42fd-b633-7dabac736dae","metadata":{},"outputs":[],"source":["from sklearn.utils import shuffle\n","\n","\n","IMG_DIM = (150, 150)\n","\n","# Read in all O and R test images file paths. Shuffle and select 50 random test images. \n","test_files_O = glob.glob('./o-vs-r-split/test/O/*')\n","test_files_R = glob.glob('./o-vs-r-split/test/R/*')\n","test_files = test_files_O + test_files_R\n","test_files = shuffle(test_files)[0:50]\n","\n","# Extract label from file path\n","test_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=IMG_DIM)) for img in test_files]\n","test_imgs = np.array(test_imgs)\n","test_labels = [fn.split('/')[3].split('.')[0].strip() for fn in test_files]\n","\n","# Standardize\n","test_imgs_scaled = test_imgs.astype('float32')\n","test_imgs_scaled /= 255"]},{"cell_type":"code","execution_count":null,"id":"af767a71-f21e-4af0-92a4-bd60885605e1","metadata":{},"outputs":[],"source":["class2num_lt = lambda l: [0 if x == 'O' else 1 for x in l]\n","num2class_lt = lambda l: ['O' if x < 0.5 else 'R' for x in l]\n","\n","test_labels_enc = class2num_lt(test_labels)"]},{"cell_type":"code","execution_count":null,"id":"eca3c3ac-d7f6-4dc3-a6bc-4a12d59f4b54","metadata":{},"outputs":[],"source":["predictions_extract_feat_model = extract_feat_model.predict(test_imgs_scaled, verbose=0)\n","predictions_fine_tune_model = fine_tune_model.predict(test_imgs_scaled, verbose=0)"]},{"cell_type":"code","execution_count":null,"id":"ce68ae3a-543c-4a8e-b45d-953e9dd02e02","metadata":{},"outputs":[],"source":["predictions_extract_feat_model = num2class_lt(predictions_extract_feat_model)\n","predictions_fine_tune_model = num2class_lt(predictions_fine_tune_model)"]},{"cell_type":"code","execution_count":null,"id":"820755b2-e201-4bff-9aba-63860303bfc0","metadata":{},"outputs":[],"source":["print('Extract Features Model')\n","print(metrics.classification_report(test_labels, predictions_extract_feat_model))\n","print('Fine-Tuned Model')\n","print(metrics.classification_report(test_labels, predictions_fine_tune_model))"]},{"cell_type":"markdown","id":"96736f40-dc9b-4f69-be44-8302f432b75c","metadata":{},"source":["#### Custom image: is your waste organic or a recyclable?\n"]},{"cell_type":"code","execution_count":null,"id":"bd2b14d9-ad06-49c0-9f34-b9d63eddc503","metadata":{},"outputs":[],"source":["custom_im = test_imgs_scaled[2]\n","plt.imshow(custom_im)\n","\n","num2class_lt(extract_feat_model.predict(custom_im.reshape((1,\n","                                                           test_imgs_scaled.shape[1], \n","                                                           test_imgs_scaled.shape[2], \n","                                                           test_imgs_scaled.shape[3])), verbose=0))"]},{"cell_type":"code","execution_count":null,"id":"715aa2d8-2a2c-44a6-ac3d-dcd662d91983","metadata":{},"outputs":[],"source":["custom_im = test_imgs_scaled[3]\n","plt.imshow(custom_im)\n","\n","num2class_lt(extract_feat_model.predict(custom_im.reshape((1,\n","                                                           test_imgs_scaled.shape[1], \n","                                                           test_imgs_scaled.shape[2], \n","                                                           test_imgs_scaled.shape[3])), verbose=0))"]},{"cell_type":"markdown","id":"0a035ef0-3ac2-44cb-89df-4437c52ebb09","metadata":{},"source":["Now we are all set to see if our waste product is an organic or a recyclable.\n"]},{"cell_type":"markdown","id":"f7dacb78-aeab-4660-82f2-49df1c0a82d5","metadata":{},"source":["## Example 2: Use Transfer Learning for identifying Stop Signs\n","\n","Other than VGG-16, there are many state-of-the-art CNN architectures we could explore, such as **InceptionNet, MobileNet, ResNet, Xception**, and so on. There are more than two dozen pre-trained models available from Keras that we could use directly for transfer learning. In this example, we will implement some of them for distinguishing stop signs. \n","\n","The stop sign datasets will be downloaded using the following cells. Compared to a typical dataset of images, the stop sign dataset is relatively small as it only contains around 200 training images and 8 test images. This is when transfer learning should come to rescue, because training models from scratch would require a lot of data, otherwise you will likely overfit your model. \n"]},{"cell_type":"markdown","id":"9b9c40f0-d5c1-4abe-9554-a2ed0e66c3b8","metadata":{},"source":["### Loading images\n"]},{"cell_type":"markdown","id":"9fe289f7-57dc-40d0-b65c-f7a57faeb76b","metadata":{},"source":["Uncomment the following cell to download the data files and unzip them.\n"]},{"cell_type":"code","execution_count":null,"id":"f0f20b6b-eda3-496a-8dfe-9889da7e9776","metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/dataset/stop.zip\",\n","                           overwrite=True)\n","await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/dataset/not_stop.zip\",\n","                           overwrite=True)\n","await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/dataset/test_set_stop_not_stop.zip\",\n","                           overwrite=True)"]},{"cell_type":"markdown","id":"a0ac51b0-fa54-4940-bf0d-4cb071b61670","metadata":{},"source":["#### Creating image directories\n","\n","We will rearange the image directories as follows:\n","\n","```python\n","signs/\n","└── train\n","    ├── stop\n","    └── not_stop\n","└── test\n","    ├── stop\n","    └── not_stop\n","```\n","\n","We will have a train and a test directory, and each contains images that have stop signs and those that don't. This setup of directories will be handy when we use the `flow_from_directory` function later to build data generators.\n"]},{"cell_type":"code","execution_count":null,"id":"3b36ca6d-6033-4a4c-abe8-285533dd9f20","metadata":{},"outputs":[],"source":["dataset_home = 'signs/'\n","subdirs = ['train/', 'test/']\n","for subdir in subdirs:\n","    labeldirs = ['stop/', 'not_stop/']\n","    for labeldir in labeldirs:\n","        newdir = dataset_home + subdir + labeldir\n","        makedirs(newdir, exist_ok = True)"]},{"cell_type":"markdown","id":"b15afaa4-1920-46e7-b3c9-2c0f87374135","metadata":{},"source":["#### Moving images\n","\n","Let's move the images according to the map above.\n"]},{"cell_type":"code","execution_count":null,"id":"e862b3b1-892a-454e-8b98-2e217e0fc9a6","metadata":{},"outputs":[],"source":["for file in listdir(\"stop\"):\n","    if file != '.DS_Store':\n","        shutil.copyfile(f\"stop/{file}\", f\"signs/train/stop/{file}\")\n","        \n","for file in listdir(\"not_stop\"):\n","    if file != '.DS_Store':\n","        shutil.copyfile(f\"not_stop/{file}\", f\"signs/train/not_stop/{file}\")\n","\n","test_path = \"test_set_stop_not_stop/\"\n","for file in listdir(test_path):\n","    if file.startswith(\"stop\"):\n","        shutil.copyfile(test_path+file, f\"signs/test/stop/{file}\")\n","    elif file.startswith(\"not_stop\"):\n","        shutil.copyfile(test_path+file, f\"signs/test/not_stop/{file}\")      \n","       "]},{"cell_type":"markdown","id":"e4f35ad4-1d12-43ed-b4d4-36dd0e9ac09a","metadata":{},"source":["#### Displaying raw images\n","\n","Use the following code to display the first five images in the train set that have stop signs and the first five images that don't have stop signs.\n"]},{"cell_type":"code","execution_count":null,"id":"309ef5cf-db2e-440c-bd09-d8e8b85724de","metadata":{},"outputs":[],"source":["train_stop = glob.glob('./signs/train/stop/*')\n","train_not_stop = glob.glob('./signs/train/not_stop/*')\n","\n","fig1, ax1 = plt.subplots(1,5,figsize=(15,4))\n","fig1.suptitle(\"STOP Signs\", fontsize=18)\n","l1 = [ax1[i].imshow(imread(train_stop[i])) for i in range(5)]\n","\n","fig2, ax2 = plt.subplots(1,5,figsize=(15,4))\n","fig2.suptitle(\"NO STOP Signs\", fontsize=18)\n","l2 = [ax2[i].imshow(imread(train_not_stop[i])) for i in range(5)]\n"]},{"cell_type":"markdown","id":"b864df9f-3bef-4efe-bf23-8f1ef06a0228","metadata":{},"source":["#### Building image data generators\n"]},{"cell_type":"markdown","id":"34e0ef8b-c458-4332-a408-59cf54ce90d8","metadata":{},"source":["Same as what we did for the waste data, we will build image data generators that perform real-time image augmentation while returning batches of image data. We will apply image augmentation on the training data only. We use a validation split of 0.2 and a batch size of 30.\n"]},{"cell_type":"code","execution_count":31,"id":"4044a6a2-4916-4811-b236-2a0898da44fd","metadata":{},"outputs":[],"source":["path = \"signs/train/\"\n","labels = ['stop', 'not_stop']\n","seed = 123\n","batch_size = 30\n","target_size = (112,112)"]},{"cell_type":"code","execution_count":32,"id":"580a248c-9760-4fcd-a25c-1b0e2371ddc8","metadata":{},"outputs":[],"source":["train_datagen = ImageDataGenerator(validation_split=0.2,\n","                                  rescale=1./255.,\n","                                  rotation_range=40,\n","                                  width_shift_range=0.2,\n","                                  height_shift_range=0.2,\n","                                  shear_range=0.2,\n","                                  zoom_range=0.2,\n","                                  horizontal_flip=True)\n","\n","val_datagen = ImageDataGenerator(validation_split=0.2,\n","                                  rescale=1./255.)\n"]},{"cell_type":"code","execution_count":33,"id":"bf300413-13b5-4820-891f-6138fb39268d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 0 images belonging to 2 classes.\n","Found 0 images belonging to 2 classes.\n"]}],"source":["train_generator = train_datagen.flow_from_directory(\n","    directory = path,\n","    classes = labels,\n","    seed = seed,\n","    batch_size = batch_size, \n","    class_mode='binary',\n","    shuffle = True,\n","    target_size=target_size,\n","    subset = 'training'\n",")\n","\n","val_generator = val_datagen.flow_from_directory(\n","    directory = path,\n","    classes = labels,\n","    seed = seed,\n","    batch_size = batch_size, \n","    class_mode='binary',\n","    shuffle = True,\n","    target_size=target_size,\n","    subset = 'validation'\n",")"]},{"cell_type":"code","execution_count":null,"id":"0f146d2a-c6dd-44fd-a3e2-b50ca05761c5","metadata":{},"outputs":[],"source":["print(train_generator.class_indices)\n","\n","prob2class = lambda x: 'Stop' if x < 0.5 else 'Not Stop' "]},{"cell_type":"markdown","id":"a68596a9-baa6-4118-b9cd-10302946e550","metadata":{},"source":["For our test images, we convert them into numpy arrays with pixel values scaled to 0-1. The test data will not be seen by the model during training or validation, but they will be used to evaluate the predictive power of our model at the end.\n"]},{"cell_type":"code","execution_count":30,"id":"1b9384c4-60d2-4137-8862-b7a544b58a2b","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'shuffle' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigns/test/stop/*.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigns/test/not_stop/*.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test_files \u001b[38;5;241m=\u001b[39m \u001b[43mshuffle\u001b[49m(test_files)\n\u001b[0;32m      4\u001b[0m test_imgs \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimg_to_array(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mload_img(img, target_size\u001b[38;5;241m=\u001b[39mtarget_size)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m test_files]\n\u001b[0;32m      5\u001b[0m test_imgs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_imgs)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'shuffle' is not defined"]}],"source":["test_files = glob.glob('signs/test/stop/*.jpeg') + glob.glob('signs/test/not_stop/*.jpeg')\n","test_files = shuffle(test_files)\n","\n","test_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=target_size)) for img in test_files]\n","test_imgs = np.array(test_imgs).astype('int')\n","\n","# Standardize\n","test_imgs_scaled = test_imgs.astype('float32')\n","test_imgs_scaled /= 255"]},{"cell_type":"markdown","id":"f7421a75-85cc-431b-99cd-6995e538c185","metadata":{},"source":["### Defining a helper function for building, compiling, and fitting CNNs\n"]},{"cell_type":"code","execution_count":26,"id":"9b1a0839-bf99-4169-a23f-6c089f30d8e5","metadata":{},"outputs":[],"source":["def build_compile_fit(basemodel):\n","    \n","    # flatten the output of the base model\n","    x = Flatten()(basemodel.output)\n","    # add a fully connected layer \n","    x = Dense(1024, activation='relu')(x)\n","    # add dropout layer for regularization\n","    x = Dropout(0.2)(x)\n","    # add final layer for classification\n","    x = Dense(1, activation='sigmoid')(x)\n","\n","    model = Model(basemodel.input, x)\n","    model.compile(optimizer = optimizers.RMSprop(learning_rate=0.0001),\n","                                                       loss='binary_crossentropy',\n","                                                       metrics=['accuracy'])\n","    callbacks = [EarlyStopping(monitor = 'loss', \n","                    patience = 5, \n","                    mode = 'min', \n","                    min_delta=0.01)]\n","\n","    model.fit(train_generator,\n","              validation_data = val_generator,\n","              steps_per_epoch=5, # num of batches in one epoch\n","              epochs=10,\n","              callbacks=callbacks)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"id":"5bd70107-8673-4f38-90ca-82c843ba4155","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"b1122584-c3ee-43ad-b95b-4c30ea577cd2","metadata":{},"source":["### Pre-trained Model 1: Inception-v3\n"]},{"cell_type":"markdown","id":"5024e17d-4f6e-44e7-88f8-77418f637fe6","metadata":{},"source":["**Inception-v3** is a successor to Inception-v1 with 24 million parameters and ran 48 layers deep. \n","\n","Instead of focusing on increasing the depth of the network, InceptionNet focuses on increasing the width and depth of the model simultaneously to attain better accuracy, while keeping the computing resources constant. \n","\n","It focuses on **parallel processing** and extraction of various feature maps concurrently using **Inception modules**, which are collections of convolutions with different filter sizes and pooling operations. The following is an illustration of the inception module in inception-v1 architecture:\n","\n","<center>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L2/img/Inception_v1_module.png\"></center>\n","\n","Picture credits to [Wichai Puarungroj](https://www.researchgate.net/profile/Wichai-Puarungroj-2)\n","\n","The inception architecture was refined in various ways, specifically for inception-v3, the following improvements are incorporated to achieve less expensive and still efficient networks:\n","\n","- Factorization Into Smaller Convolutions\n","- Factorization Into Asymmetric Convolutions\n","- Auxiliary Classifier used as regularizer\n","- Efficient Grid Size Reduction\n","\n","You can read more about the different versions of InceptionNet [here](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n"]},{"cell_type":"code","execution_count":null,"id":"c458939d-198b-48ce-a8cd-051c62e5f36c","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"353db0bf-5795-42ba-bea1-2643cd51ff70","metadata":{},"source":["Let's import the pre-trained Inception-v3 architecture from keras applications for our transfer learning task:\n"]},{"cell_type":"code","execution_count":28,"id":"41207865-2fb8-40d8-b158-457b44e49fe0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 48s/step - accuracy: 0.5595 - loss: 7.1360 - val_accuracy: 0.5537 - val_loss: 2.3835\n","Epoch 2/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7s/step - accuracy: 0.5869 - loss: 1.8080 - val_accuracy: 0.7707 - val_loss: 0.6218\n","Epoch 3/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8s/step - accuracy: 0.6398 - loss: 0.8930 - val_accuracy: 0.8292 - val_loss: 0.3668\n","Epoch 4/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9s/step - accuracy: 0.8087 - loss: 0.4695 - val_accuracy: 0.8953 - val_loss: 0.2366\n","Epoch 5/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8s/step - accuracy: 0.8177 - loss: 0.4195 - val_accuracy: 0.6715 - val_loss: 0.9088\n","Epoch 6/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7s/step - accuracy: 0.7703 - loss: 0.6194 - val_accuracy: 0.9056 - val_loss: 0.2185\n","Epoch 7/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7s/step - accuracy: 0.8409 - loss: 0.3772 - val_accuracy: 0.8547 - val_loss: 0.3240\n","Epoch 8/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8s/step - accuracy: 0.8529 - loss: 0.6273 - val_accuracy: 0.8251 - val_loss: 0.4296\n","Epoch 9/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8s/step - accuracy: 0.6862 - loss: 0.8776 - val_accuracy: 0.9105 - val_loss: 0.2261\n","Epoch 10/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 8s/step - accuracy: 0.8355 - loss: 0.3461 - val_accuracy: 0.8788 - val_loss: 0.2755\n"]}],"source":["from keras.applications.inception_v3 import InceptionV3\n","\n","# initialize the base model\n","basemodel = InceptionV3(input_shape=(150,150,3),\n","                          include_top = False,\n","                          weights = 'imagenet')\n","\n","for layer in basemodel.layers:\n","    layer.trainable = False\n","\n","# call the build_compile_fit function to complete model training\n","inception_v3 = build_compile_fit(basemodel)"]},{"cell_type":"markdown","id":"db0e5f1b-8b04-4f86-a8b6-0a257cd0d0bc","metadata":{},"source":["The pre-trained part of our **inception_v3** model utilizes the weights obtained from the imagenet dataset training. Only the layers that we added are trained on the stop sign's data.\n","\n","Let's now display the test images along with their class labels predicted by the fitted **inception_v3**:\n"]},{"cell_type":"code","execution_count":29,"id":"49cec020-fca9-4f29-a9f8-2d698b49f129","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'test_imgs' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ax\u001b[38;5;241m.\u001b[39mflat):\n\u001b[1;32m----> 4\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mtest_imgs\u001b[49m[i])\n\u001b[0;32m      5\u001b[0m     pred_class \u001b[38;5;241m=\u001b[39m prob2class(inception_v3\u001b[38;5;241m.\u001b[39mpredict(test_imgs_scaled[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m,\u001b[38;5;241m3\u001b[39m)))\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# print the predicted class label as the title of the image\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'test_imgs' is not defined"]},{"data":{"image/png":"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","text/plain":["<Figure size 1200x600 with 8 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n","\n","for i, ax in enumerate(ax.flat):\n","    ax.imshow(test_imgs[i])\n","    pred_class = prob2class(inception_v3.predict(test_imgs_scaled[i].reshape(1, 112, 112,3)))\n","    \n","    # print the predicted class label as the title of the image\n","    ax.set_title(pred_class, fontsize=15)\n","    ax.axis(\"off\")"]},{"cell_type":"markdown","id":"6cb76cc9-bbe4-4bf5-9618-e7a2926a6aef","metadata":{},"source":["### Pre-trained Model 2: MobileNet\n"]},{"cell_type":"markdown","id":"164f6396-dc66-48b1-b7fb-b223674dbf55","metadata":{},"source":["**MobileNets** are very efficient and small deep learning architectures specially designed for mobile devices.\n","\n","It uses of a new kind of convolution layer, known as **Depthwise Separable convolution**. The main difference between a 2D convolution and a Depthwise convolution is that the former is performed over multiple input channels by doing a weighted sum of the input pixels with the filter, whereas the latter is performed separately over each channel. \n","\n","For example, if the input image has three channels, then the output of depthwise separable convolution will also have three channels. The next step is **Pointwise convolution**, which is similar to a regular convolution with a $1\\times1$ filter. By doing so, we can again merge the three channels from depthwise separable convolution into one to create new features.\n"]},{"cell_type":"markdown","id":"1f9358c8-43d3-45bd-b17f-3bda1b986230","metadata":{},"source":["Let's import the pre-trained MobileNet architecture from keras applications for our transfer learning task:\n"]},{"cell_type":"code","execution_count":null,"id":"e5602e82-5c76-4ccc-b177-9834e45ae320","metadata":{},"outputs":[],"source":["from keras.applications.mobilenet import MobileNet\n","\n","# initialize the base model\n","basemodel = MobileNet(input_shape=(112,112,3),\n","                          include_top = False,\n","                          weights = 'imagenet')\n","\n","for layer in basemodel.layers:\n","    layer.trainable = False\n","    \n","# call the build_compile_fit function to complete model training\n","mobile_net = build_compile_fit(basemodel)"]},{"cell_type":"markdown","id":"1c1e8d54-31f3-4d07-97a6-d753178a26ef","metadata":{},"source":["Let's display the test images along with the their class labels predicted by the fitted **mobile_net**:\n"]},{"cell_type":"code","execution_count":null,"id":"62e91eac-f50d-4a96-ab09-3943b9fc10d1","metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n","\n","for i, ax in enumerate(ax.flat):\n","    ax.imshow(test_imgs[i])\n","    pred_class = prob2class(mobile_net.predict(test_imgs_scaled[i].reshape(1, 112, 112,3)))\n","    \n","    # print the predicted class label as the title of the image\n","    ax.set_title(pred_class, fontsize=15)\n","    ax.axis(\"off\")"]},{"cell_type":"markdown","id":"16876591-c269-4c2a-8957-fd49bf6b8335","metadata":{},"source":["### Pre-trained Model 3: ResNet-50\n"]},{"cell_type":"markdown","id":"e8478084-aa14-4b9e-a26b-d150c415046c","metadata":{},"source":["**ResNet** features special skip connections which add the output from an earlier layer directly to a later layer and heavy use of batch normalization. It allows us to design deep CNNs without compromising the model’s convergence and accuracy. The basic building blocks for ResNets are the convolution and identity blocks. \n","\n","Essentially, ResNet uses the network layers to fit a residual mapping $F(x) + x$, instead of trying to learn the desired underlying mapping $H(x)$ directly with stacked layers. \n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module3/L2/img/resnet.png\" width=\"30%\"></center>\n","\n","The formulation of $F(x) + x$ can be realized by the feedforward neural networks with **shortcut connections**. Shortcut connections add the output from the previous layer directly to the output of the current layer, which can be seen as **identity mapping**.\n","\n","By doing so, the network overcomes the vanishing gradient problem because now the gradient signals could travel back to early layers through this shortcut.\n"]},{"cell_type":"markdown","id":"7ca968de-4a9f-4107-8b34-5f17a11fd33e","metadata":{},"source":["Similarly, let's import the pre-trained ResNet-50 architecture from keras applications.\n"]},{"cell_type":"code","execution_count":null,"id":"f75ec843-785e-4c2c-b132-a1f00f5d4ebb","metadata":{},"outputs":[],"source":["from keras.applications import ResNet50\n","\n","# initialize the base model\n","basemodel = ResNet50(input_shape=(112,112,3),\n","                          include_top = False,\n","                          weights = 'imagenet')\n","\n","for layer in basemodel.layers:\n","    layer.trainable = False\n","\n","# call the build_compile_fit function to complete model training\n","resnet_50 = build_compile_fit(basemodel)"]},{"cell_type":"markdown","id":"d88760c6-86a8-4ce1-b886-c61cdd16ae87","metadata":{},"source":["Let's display the test images along with their class labels predicted by **resnet_50**:\n"]},{"cell_type":"code","execution_count":null,"id":"31235cd0-4212-42eb-ab0f-5488b75dc807","metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n","\n","for i, ax in enumerate(ax.flat):\n","    ax.imshow(test_imgs[i])\n","    pred_class = prob2class(resnet_50.predict(test_imgs_scaled[i].reshape(1, 112, 112,3)))\n","    \n","    # print the predicted class label as the title of the image\n","    ax.set_title(pred_class, fontsize=15)\n","    ax.axis(\"off\")"]},{"cell_type":"markdown","id":"8f06cb00-fec7-4ff9-897f-24fededfdde9","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"a3b54ab9-ca91-47e5-8d95-b7e4df0208b9","metadata":{},"source":["[Kopal Garg](https://www.linkedin.com/in/gargkopal/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01)\n"]},{"cell_type":"markdown","id":"12eb5023-1ab2-4a43-9629-417e93c45272","metadata":{},"source":["Kopal Garg is a Masters student in Computer Science at the University of Toronto.\n"]},{"cell_type":"markdown","id":"89f9ed29-aa80-4bb9-952d-ac6844b0bad6","metadata":{},"source":["[Roxanne Li](https://www.linkedin.com/in/roxanne-li/) is a Data Science intern at IBM Skills Network, entering level-5 study in the Mathematics & Statistics undergraduate Coop program at McMaster University.\n"]},{"cell_type":"markdown","id":"361913ca-e634-414d-a610-e08b92123afc","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","id":"abbc58e8-13ba-4867-abd4-52d106648289","metadata":{},"source":["| Date (YYYY-MM-DD) | Version | Changed By  | Change Description |\n","| ----------------- | ------- | ----------- | ------------------ |\n","| 2022-05-31        | 0.1     | Kopal Garg  | Created Lab        |\n","| 2022-06-28        | 0.1     | Roxanne Li  | Added examples     |\n","| 2022-09-07        | 0.1     | Steve Hord  | QA pass edits      |\n"]},{"cell_type":"code","execution_count":null,"id":"47ae4442-2e8f-4961-a5f0-21dc3c9f3f2d","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"prev_pub_hash":"eb16dd8ca4216333f71b5267e56ce2b100b8476d7a5bf8c58fff12fed5ff54bc"},"nbformat":4,"nbformat_minor":4}
