{"cells":[{"cell_type":"markdown","id":"12ec4e02-9c8f-4830-9d88-5e5f11538165","metadata":{},"source":["# Machine Learning Foundation\n","\n","## Section 2, Part c: Cross Validation \n"]},{"cell_type":"markdown","id":"4508ca30-2138-4049-a016-51d851750f78","metadata":{},"source":["## Learning objectives\n","\n","By the end of this lesson, you will be able to:\n","\n","* Chain multiple data processing steps together using `Pipeline`\n","* Use the `KFolds` object to split data into multiple folds.\n","* Perform cross validation using SciKit Learn with `cross_val_predict` and `GridSearchCV`\n"]},{"cell_type":"code","execution_count":1,"id":"fb6b84a6-ef92-461b-8501-e9fdcb886889","metadata":{},"outputs":[],"source":["\n","import numpy as np\n","import pickle\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.model_selection import KFold, cross_val_predict\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge\n","from sklearn.metrics import r2_score\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"code","execution_count":3,"id":"e74895c8-2fe0-461a-bb48-16604b6d831a","metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'boston_housing_clean.pickle'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# boston = pickle.load(open('data/boston_housing_clean.pickle', \"rb\" ))\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m boston \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mboston_housing_clean.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[1;32mc:\\GitHub\\IBM_Machine_Learning\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'boston_housing_clean.pickle'"]}],"source":["# boston = pickle.load(open('data/boston_housing_clean.pickle', \"rb\" ))\n","boston = pickle.load(open('boston_housing_clean.pickle', \"rb\" ))"]},{"cell_type":"code","execution_count":null,"id":"54e98d31-1702-4e5e-8792-cff1719362ea","metadata":{},"outputs":[],"source":["boston.keys()"]},{"cell_type":"code","execution_count":null,"id":"87cacd66-5eb7-4ef0-9586-5c0c9b6eb7dc","metadata":{},"outputs":[],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"id":"d852887b-25a7-407e-9d30-2c27c97cbfeb","metadata":{},"outputs":[],"source":["boston_data = boston['dataframe']\n","boston_description = boston['description']"]},{"cell_type":"code","execution_count":null,"id":"e4c71176-ec0e-46c6-9151-e3722d875692","metadata":{},"outputs":[],"source":["boston_data.head()"]},{"cell_type":"markdown","id":"dd1b3bc6-439e-475c-8d20-4a834f1e4781","metadata":{},"source":["### Discussion: \n","\n","Suppose we want to do Linear Regression on our dataset to get an estimate, based on mean squared error, of how well our model will perform on data outside our dataset. \n","\n","Suppose also that our data is split into three folds: Fold 1, Fold 2, and Fold 3.\n","\n","What would the steps be, in English, to do this?\n"]},{"cell_type":"markdown","id":"93544255-18bf-4186-a647-a3a6267546c9","metadata":{},"source":["**Your response below**\n"]},{"cell_type":"markdown","id":"dd8cb8f1-4f7f-49b8-b884-0442b21c9c03","metadata":{},"source":[" \n"]},{"cell_type":"markdown","id":"8c1e18c0-ce68-4841-b961-cffcb83b80e3","metadata":{},"source":["#### Coding this up\n","\n","The [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) object in SciKit Learn tells the cross validation object (see below) how to split up the data:\n"]},{"cell_type":"code","execution_count":null,"id":"b414454e-b4b4-4c15-822e-7de7e601858e","metadata":{},"outputs":[],"source":["X = boston_data.drop('MEDV', axis=1)\n","y = boston_data.MEDV"]},{"cell_type":"code","execution_count":null,"id":"cbe069fd-4f44-4600-9e36-bcec2a9a3da7","metadata":{},"outputs":[],"source":["kf = KFold(shuffle=True, random_state=72018, n_splits=3)"]},{"cell_type":"code","execution_count":null,"id":"342508b2-a620-4b0f-9632-738b9a959352","metadata":{},"outputs":[],"source":["for train_index, test_index in kf.split(X):\n","    print(\"Train index:\", train_index[:10], len(train_index))\n","    print(\"Test index:\",test_index[:10], len(test_index))\n","    print('')"]},{"cell_type":"code","execution_count":null,"id":"b943e6fe-c046-48f1-b66e-af350eabc77e","metadata":{},"outputs":[],"source":["#from sklearn.metrics import r2_score, mean_squared_error\n","\n","scores = []\n","lr = LinearRegression()\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n","                                        X.iloc[test_index, :], \n","                                        y[train_index], \n","                                        y[test_index])\n","    \n","    lr.fit(X_train, y_train)\n","        \n","    y_pred = lr.predict(X_test)\n","\n","    score = r2_score(y_test.values, y_pred)\n","    \n","    scores.append(score)\n","    \n","scores"]},{"cell_type":"markdown","id":"5170a4a4-ae31-4aaf-b875-6faacd355d6a","metadata":{},"source":["A bit cumbersome, but do-able.\n"]},{"cell_type":"markdown","id":"eafb8410-cddc-499f-8089-cc66d4e8b0c8","metadata":{},"source":["### Discussion (Part 2): \n","\n","Now suppose we want to do the same, but appropriately scaling our data as we go through the folds.\n","\n","What would the steps be _now_?\n"]},{"cell_type":"markdown","id":"b4e2bfc3-4422-4261-a343-b96619c4d20a","metadata":{},"source":["**Your response below**\n"]},{"cell_type":"markdown","id":"d2704c55-e505-41fa-be0c-55d37e9acf1e","metadata":{},"source":["### Coding this up\n"]},{"cell_type":"code","execution_count":null,"id":"44c69e8e-00d2-4cde-9aa7-cbf4b6e4ff3e","metadata":{},"outputs":[],"source":["scores = []\n","\n","lr = LinearRegression()\n","s = StandardScaler()\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n","                                        X.iloc[test_index, :], \n","                                        y[train_index], \n","                                        y[test_index])\n","    \n","    X_train_s = s.fit_transform(X_train)\n","    \n","    lr.fit(X_train_s, y_train)\n","    \n","    X_test_s = s.transform(X_test)\n","    \n","    y_pred = lr.predict(X_test_s)\n","\n","    score = r2_score(y_test.values, y_pred)\n","    \n","    scores.append(score)"]},{"cell_type":"code","execution_count":null,"id":"681b7bfd-0dde-4899-abe7-7fb112860b07","metadata":{},"outputs":[],"source":["scores"]},{"cell_type":"markdown","id":"5f0e37bf-2e13-4149-82e5-a1eb798981e9","metadata":{},"source":["(same scores, because for vanilla linear regression with no regularization, scaling actually doesn't matter for performance)\n"]},{"cell_type":"markdown","id":"0bdbad18-f91b-4996-9c0d-48f461acd643","metadata":{},"source":["This is getting quite cumbersome! \n","\n","_Very_ luckily, SciKit Learn has some wonderful functions that handle a lot of this for us.\n"]},{"cell_type":"markdown","id":"7e26cde3-8256-4f75-8b43-216bafc808c1","metadata":{},"source":["### `Pipeline` and `cross_val_predict`\n"]},{"cell_type":"markdown","id":"1b1d4b2d-4070-4c43-a52c-ac3adb2d6da1","metadata":{},"source":["`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method.\n"]},{"cell_type":"code","execution_count":null,"id":"8d4b5fae-afaf-4cff-a74f-b2b94da3fb98","metadata":{},"outputs":[],"source":["s = StandardScaler()\n","lr = LinearRegression()"]},{"cell_type":"markdown","id":"ca3cad34-87fb-4f7c-b8cd-e18e1604f45e","metadata":{},"source":["### Combine multiple processing steps into a `Pipeline`\n","\n","A pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.  \n"]},{"cell_type":"code","execution_count":null,"id":"659a5607-58c6-404f-8223-04be928d575e","metadata":{},"outputs":[],"source":["estimator = Pipeline([(\"scaler\", s),\n","                      (\"regression\", lr)])"]},{"cell_type":"markdown","id":"e59b73e2-b8c0-4572-8c68-d2325258917f","metadata":{},"source":["### `cross_val_predict`\n","\n","[`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way.\n"]},{"cell_type":"code","execution_count":null,"id":"4d5ef4f4-0a22-4544-abdf-3eb99993b585","metadata":{},"outputs":[],"source":["kf"]},{"cell_type":"code","execution_count":null,"id":"d4ff0d96-fe79-4357-863d-6b2d526e6442","metadata":{},"outputs":[],"source":["predictions = cross_val_predict(estimator, X, y, cv=kf)"]},{"cell_type":"code","execution_count":null,"id":"1d5afa8d-3348-4c53-86b0-35049dfcd6dc","metadata":{},"outputs":[],"source":["r2_score(y, predictions)"]},{"cell_type":"code","execution_count":null,"id":"a089f671-e738-4af9-837e-c3550069de1f","metadata":{},"outputs":[],"source":["np.mean(scores) # almost identical!"]},{"cell_type":"markdown","id":"6a32bab2-4bfd-43f6-80ef-3cb895493a2d","metadata":{},"source":["Note that `cross_val_predict` doesn't use the same model for all steps; the predictions for each row are made when that row is in the validation set. We really have the collected results of 3 (i.e. `kf.num_splits`) different models. \n","\n","When we are done, `estimator` is still not fitted. If we want to predict on _new_ data, we still have to train our `estimator`. \n"]},{"cell_type":"markdown","id":"00f5dc44-32a7-4c1c-8b3c-6035eaaa017c","metadata":{},"source":["## Hyperparameter tuning\n"]},{"cell_type":"markdown","id":"23c1a676-2f42-41a4-9b4d-aefa2e098e15","metadata":{},"source":["### Definition\n","\n","**Hyperparameter tuning** involves using cross validation (or train-test split) to determine which hyperparameters are most likely to generate a model that _generalizes_ well outside of your sample.\n","\n","### Mechanics\n","\n","We can generate an exponentially spaces range of values using the numpy [`geomspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html#numpy.geomspace) function.\n","\n","```python\n","np.geomspace(1, 1000, num=4)\n","```\n","\n","produces:\n","\n","```\n","array([    1.,    10.,   100.,  1000.])\n","```\n","\n","Use this function to generate a list of length 10 called `alphas` for hyperparameter tuning:\n"]},{"cell_type":"code","execution_count":null,"id":"1e924295-d95f-4db0-b995-b4209a94bae3","metadata":{},"outputs":[],"source":["alphas = np.geomspace(1e-9, 1e0, num=10)\n","alphas"]},{"cell_type":"markdown","id":"d3d5a4eb-e9e3-4a12-95d5-f9dcfe20ce19","metadata":{},"source":["The code below tunes the `alpha` hyperparameter for Lasso regression.\n"]},{"cell_type":"code","execution_count":null,"id":"76f4064d-b0ac-4a6e-a07b-7d94ef7c5264","metadata":{},"outputs":[],"source":["scores = []\n","coefs = []\n","for alpha in alphas:\n","    las = Lasso(alpha=alpha, max_iter=100000)\n","    \n","    estimator = Pipeline([\n","        (\"scaler\", s),\n","        (\"lasso_regression\", las)])\n","\n","    predictions = cross_val_predict(estimator, X, y, cv = kf)\n","    \n","    score = r2_score(y, predictions)\n","    \n","    scores.append(score)"]},{"cell_type":"code","execution_count":null,"id":"8e86b541-0a48-43e1-8036-f0bda3e3d19a","metadata":{},"outputs":[],"source":["list(zip(alphas,scores))"]},{"cell_type":"code","execution_count":null,"id":"41845885-4b0e-45ab-b7c0-77be9b59746d","metadata":{},"outputs":[],"source":["Lasso(alpha=1e-6).fit(X, y).coef_"]},{"cell_type":"code","execution_count":null,"id":"14d13291-e83e-498e-a7ee-16679b5cdb3b","metadata":{},"outputs":[],"source":["Lasso(alpha=1.0).fit(X, y).coef_"]},{"cell_type":"code","execution_count":null,"id":"d0e2e93a-cff9-4510-b0cd-1b06692b9852","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,6))\n","plt.semilogx(alphas, scores, '-o')\n","plt.xlabel('$\\\\alpha$')\n","plt.ylabel('$R^2$');"]},{"cell_type":"markdown","id":"f4e7136f-7ecf-4ba8-8921-abe389a6f772","metadata":{},"source":["### Exercise\n","\n","Add `PolynomialFeatures` to this `Pipeline`, and re-run the cross validation with the `PolynomialFeatures` added.\n","\n","**Hint #1:** pipelines process input from first to last. Think about the order that it would make sense to add Polynomial Features to the data in sequence and add them in the appropriate place in the pipeline.\n","\n","**Hint #2:** you should see a significant increase in cross validation accuracy from doing this\n"]},{"cell_type":"code","execution_count":null,"id":"ccc6f99c-a5f6-4bec-8696-faba60ededac","metadata":{},"outputs":[],"source":["pf = PolynomialFeatures(degree=2)\n","scores = []\n","\n","alphas = np.geomspace(0.001, 10, 5)\n","for alpha in alphas:\n","    las = Lasso(alpha=alpha, max_iter=100000)\n","    estimator = Pipeline([\n","        (\"make_higher_degree\", pf),\n","        (\"scaler\", s),\n","        (\"lasso_regression\", las)])\n","    predictions = cross_val_predict(estimator, X, y, cv = kf)\n","    score = r2_score(y, predictions)\n","    \n","    scores.append(score)\n","scores"]},{"cell_type":"markdown","id":"b2f75c19-2e6a-4644-aa61-4318f6545e43","metadata":{},"source":["If you store the results in a list called `scores`, the following will work:\n"]},{"cell_type":"code","execution_count":null,"id":"a9c7ca3b-59fa-4362-b27e-a272ba126ec4","metadata":{},"outputs":[],"source":["plt.semilogx(alphas, scores);"]},{"cell_type":"code","execution_count":null,"id":"035ad8e9-1ba3-4e41-a9e9-b54d4103c610","metadata":{},"outputs":[],"source":["# Once we have found the hyperparameter (alpha~1e-2=0.01)\n","# make the model and train it on ALL the data\n","# Then release it into the wild .....\n","best_estimator = Pipeline([\n","                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n","                    (\"scaler\", s),\n","                    (\"lasso_regression\", Lasso(alpha=0.01, max_iter=10000))])\n","\n","best_estimator.fit(X, y)\n","best_estimator.score(X, y)"]},{"cell_type":"code","execution_count":null,"id":"0c20acfd-4f0e-4c5d-b20d-5eef0b60d777","metadata":{},"outputs":[],"source":["best_estimator.named_steps[\"lasso_regression\"].coef_"]},{"cell_type":"markdown","id":"4d5124bb-24a1-4c4d-bb42-85961bbe4662","metadata":{},"source":["### Exercise\n","\n","Do the same, but with `Ridge` regression \n","\n","Which model, `Ridge` or `Lasso`, performs best with its optimal hyperparameters on the Boston dataset?\n"]},{"cell_type":"code","execution_count":null,"id":"9acfc597-8bef-4e7d-8129-c2ffff7298f4","metadata":{},"outputs":[],"source":["pf = PolynomialFeatures(degree=2)\n","scores = []\n","\n","alphas = np.geomspace(4, 20, 20)\n","for alpha in alphas:\n","    ridge = Ridge(alpha=alpha, max_iter=100000)\n","\n","    estimator = Pipeline([\n","        (\"polynomial_features\", pf),\n","        (\"scaler\", s),\n","        (\"ridge_regression\", ridge)])\n","\n","    predictions = cross_val_predict(estimator, X, y, cv = kf)\n","    score = r2_score(y, predictions)\n","    scores.append(score)\n","plt.plot(alphas, scores)"]},{"cell_type":"markdown","id":"5ac69437-cea5-43f0-a0a6-582f0e53a312","metadata":{},"source":["**Conclusion:** Both Lasso and Ridge with proper hyperparameter tuning give better results than plain ol' Linear Regression!\n"]},{"cell_type":"markdown","id":"33127b0c-53b4-4c21-91a2-72cbdaa8a388","metadata":{},"source":["### Exercise:\n"]},{"cell_type":"markdown","id":"2eda7ba6-9ad7-4616-b964-760ea3174c7d","metadata":{},"source":["Now, for whatever your best overall hyperparameter was: \n","\n","* Standardize the data\n","* Fit and predict on the entire dataset\n","* See what the largest coefficients were\n","    * Hint: use \n","    ```python\n","    dict(zip(model.coef_, pf.get_feature_names()))\n","    ```\n","    for your model `model` to get the feature names from `PolynomialFeatures`.\n","    \n","    Then, use\n","    ```python\n","    dict(zip(list(range(len(X.columns.values))), X.columns.values))\n","    ```\n","    \n","    to see which features in the `PolynomialFeatures` DataFrame correspond to which columns in the original DataFrame.\n"]},{"cell_type":"code","execution_count":null,"id":"cff7875d-2a9e-48a1-a646-1e849b23c626","metadata":{},"outputs":[],"source":["# Once we have found the hyperparameter (alpha~1e-2=0.01)\n","# make the model and train it on ALL the data\n","# Then release it into the wild .....\n","best_estimator = Pipeline([\n","                    (\"make_higher_degree\", PolynomialFeatures(degree=2, include_bias=False)),\n","                    (\"scaler\", s),\n","                    (\"lasso_regression\", Lasso(alpha=0.01, max_iter=10000))])\n","\n","best_estimator.fit(X, y)\n","best_estimator.score(X, y)"]},{"cell_type":"code","execution_count":null,"id":"04d807c1-4da5-4540-b539-446b8598a4c9","metadata":{},"outputs":[],"source":["df_importances = pd.DataFrame(zip(best_estimator.named_steps[\"make_higher_degree\"].get_feature_names(),\n","                 best_estimator.named_steps[\"lasso_regression\"].coef_,\n","))"]},{"cell_type":"code","execution_count":null,"id":"fa9f4f84-ca0c-49e7-a611-991f099067de","metadata":{},"outputs":[],"source":["col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))"]},{"cell_type":"code","execution_count":null,"id":"0ca93439-3c92-45e8-9e65-5fa40506b3d7","metadata":{},"outputs":[],"source":["col_names_dict"]},{"cell_type":"code","execution_count":null,"id":"5a3d28b7-0c15-4f15-af38-b243b3ade6b6","metadata":{},"outputs":[],"source":["df_importances.sort_values(by=1)"]},{"cell_type":"markdown","id":"c50ef201-15b4-4f9b-9c61-9b81cf02f699","metadata":{},"source":["## Grid Search CV\n"]},{"cell_type":"markdown","id":"65aa65b5-e923-4139-86fe-867b8c75725d","metadata":{},"source":["To do cross-validation, we used two techniques:\n","- use `KFolds` and manually create a loop to do cross-validation\n","- use `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.\n","\n","To do hyper-parameter tuning, we see a general pattern:\n","- use `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.\n","\n","Perhaps not surprisingly, there is a function that does this for us -- `GridSearchCV`\n"]},{"cell_type":"code","execution_count":null,"id":"98c0d6f5-d3c1-4ecd-8a33-154d9ea06972","metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# Same estimator as before\n","estimator = Pipeline([(\"polynomial_features\", PolynomialFeatures()),\n","        (\"scaler\", StandardScaler()),\n","        (\"ridge_regression\", Ridge())])\n","\n","params = {\n","    'polynomial_features__degree': [1, 2, 3],\n","    'ridge_regression__alpha': np.geomspace(4, 20, 20)\n","}\n","\n","grid = GridSearchCV(estimator, params, cv=kf)"]},{"cell_type":"code","execution_count":null,"id":"14d96e07-c899-406c-b4f2-ac2070c1b13b","metadata":{},"outputs":[],"source":["grid.fit(X, y)"]},{"cell_type":"code","execution_count":null,"id":"e00af536-b0eb-496e-82e7-0aa5f92e20c4","metadata":{},"outputs":[],"source":["grid.best_score_, grid.best_params_"]},{"cell_type":"code","execution_count":null,"id":"fb7c6636-9036-4126-bd04-fa2c80d72353","metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import (StandardScaler, \n","                                   PolynomialFeatures)\n","from scipy.stats.mstats import normaltest\n","from scipy.stats import boxcox\n","from scipy.special import inv_boxcox\n","\n","file_name='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ST0151EN-SkillsNetwork/labs/boston_housing.csv'\n","boston_data = pd.read_csv(file_name)\n","\n","lr = LinearRegression()\n","y_col = \"MEDV\"\n","X = boston_data.drop(y_col, axis=1)\n","y = boston_data[y_col]\n","\n","pf = PolynomialFeatures(degree=2, include_bias=False)\n","X_pf = pf.fit_transform(X)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3, \n","                                                    random_state=72018)\n","\n","s = StandardScaler()\n","X_train_s = s.fit_transform(X_train)\n","\n","bc_result = boxcox(y_train)\n","y_train_bc = bc_result[0]\n","lam = bc_result[1]\n","\n","lr.fit(X_train_s, y_train_bc)\n","X_test_s = s.transform(X_test)\n","y_pred_bc = lr.predict(X_test_s)\n","\n","y_pred_tran = inv_boxcox(y_pred_bc, lam)\n","print(r2_score(y_pred_tran,y_test)) #RES 0.848052537981275\n","\n","lr = LinearRegression()\n","lr.fit(X_train_s,y_train)\n","lr_pred = lr.predict(X_test_s)\n","r2_score(lr_pred,y_test) #RES 0.8667029116056716\n"]},{"cell_type":"code","execution_count":null,"id":"c135839d-98e5-466f-83b2-efae370cd0da","metadata":{},"outputs":[],"source":["y_predict = grid.predict(X)"]},{"cell_type":"code","execution_count":null,"id":"2646b1f8-ce1d-4f3e-840b-b49886cdeb34","metadata":{},"outputs":[],"source":["# This includes both in-sample and out-of-sample\n","r2_score(y, y_predict)"]},{"cell_type":"code","execution_count":null,"id":"1f1cc668-7e6b-47e8-a702-c28d83796b37","metadata":{},"outputs":[],"source":["# Notice that \"grid\" is a fit object!\n","# We can use grid.predict(X_test) to get brand new predictions!\n","grid.best_estimator_.named_steps['ridge_regression'].coef_"]},{"cell_type":"code","execution_count":null,"id":"871853a2-780b-4ea7-ace8-3a9aa1dc6833","metadata":{},"outputs":[],"source":["grid.cv_results_"]},{"cell_type":"markdown","id":"b08e2e4b-f86f-4b33-967f-126333baa281","metadata":{},"source":["## Summary\n","\n","1. We can manually generate folds by using `KFolds`\n","2. We can get a score using `cross_val_predict(X, y, cv=KFoldObject_or_integer)`. \n","   This will produce the out-of-bag prediction for each row.\n","3. When doing hyperparameter selection, we should be optimizing on out-of-bag scores. This means either using `cross_val_predict` in a loop, or ....\n","4. .... use `GridSearchCV`. GridSearchCV takes a model (or pipeline) and a dictionary of parameters to scan over. It finds the hyperparameter set that has the best out-of-sample score on all the parameters, and calls that it's \"best estimator\". It then retrains on all data with the \"best\" hyper-parameters.\n","\n","### Extensions\n","\n","Here are some additional items to keep in mind:\n","* There is a `RandomSearchCV` that tries random combination of model parameters. This can be helpful if you have a prohibitive number of combinations to test them all exhaustively.\n","* KFolds will randomly select rows to be in the training and test folds. There are other methods (such as `StratifiedKFolds` and `GroupKFold`, which are useful when you need more control over how the data is split (e.g. to prevent data leakage). You can create these specialized objects and pass them to the `cv` argument of `GridSearchCV`.\n"]},{"cell_type":"markdown","id":"49e1880e-cdc9-4019-9855-f4c6372f8ef8","metadata":{},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
